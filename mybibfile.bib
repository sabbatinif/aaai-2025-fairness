%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Roberta Calegari at 2024-08-08 20:06:23 +0200 


%% Saved with string encoding Unicode (UTF-8) 



@article{bostrom2020ethical,
	author = {Bostrom, Nick},
	date-added = {2024-08-08 20:06:21 +0200},
	date-modified = {2024-08-08 20:06:21 +0200},
	journal = {Machine Ethics and Robot Ethics},
	pages = {69--75},
	publisher = {Routledge},
	title = {Ethical issues in advanced artificial intelligence},
	year = {2020}}

@article{cao2020ai,
	author = {Cao, Longbing},
	date-added = {2024-08-08 19:58:23 +0200},
	date-modified = {2024-08-08 19:58:23 +0200},
	journal = {Available at SSRN 3647625},
	title = {AI in finance: A review},
	year = {2020}}

@inproceedings{NIPS2017_a486cd07,
	author = {Kusner, Matt J. and Loftus, Joshua and Russell, Chris and Silva, Ricardo},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {Counterfactual Fairness},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf},
	volume = {30},
	year = {2017},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2017/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf}}

@book{pearl2009causality,
	author = {Pearl, Judea},
	publisher = {Cambridge University Press},
	title = {Causality: Models, Reasoning and Inference.},
	year = {2000}}

@inbook{doi:https://doi.org/10.1002/9781118619179.ch3,
	abstract = {Summary This chapter explores the nature of causality, the conditions of causation, and the limits of causal modeling. It examines threats to isolation or pseudo-isolation. The chapter describes some of the limits of structural equation modeling. The three subsections are (1) model-data versus model-reality consistency, (2) experimental and nonexperimental research designs, and (3) criticisms of structural equation modeling. Two common misconceptions about structural equation models are (1) that these are statistical techniques only for nonexperimental data and (2) that true experiments solve the omitted variable problem that plagues structural equation analyses of nonexperimental data. Finally, the chapter realizes that the problems of demonstrating isolation, association, and direction of causation are age-old issues. The chapter describes them as they are manifested in structural equation models.},
	author = {Bollen, Kenneth A.},
	booktitle = {Structural Equations with Latent Variables},
	chapter = {Three},
	doi = {https://doi.org/10.1002/9781118619179.ch3},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118619179.ch3},
	isbn = {9781118619179},
	keywords = {causal models, pseudo-isolation},
	pages = {40-79},
	publisher = {John Wiley \& Sons, Ltd},
	title = {Causality and Causal Models},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781118619179.ch3},
	year = {1989},
	bdsk-url-1 = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781118619179.ch3},
	bdsk-url-2 = {https://doi.org/10.1002/9781118619179.ch3}}

@article{10.1145/3527848,
	abstract = {Machine learning is increasingly used to inform decision making in sensitive situations where decisions have consequential effects on individuals' lives. In these settings, in addition to requiring models to be accurate and robust, socially relevant values such as fairness, privacy, accountability, and explainability play an important role in the adoption and impact of said technologies. In this work, we focus on algorithmic recourse, which is concerned with providing explanations and recommendations to individuals who are unfavorably treated by automated decision-making systems. We first perform an extensive literature review, and align the efforts of many authors by presenting unified definitions, formulations, and solutions to recourse. Then, we provide an overview of the prospective research directions toward which the community may engage, challenging existing assumptions and making explicit connections to other ethical challenges such as security, privacy, and fairness.},
	address = {New York, NY, USA},
	articleno = {95},
	author = {Karimi, Amir-Hossein and Barthe, Gilles and Sch\"{o}lkopf, Bernhard and Valera, Isabel},
	doi = {10.1145/3527848},
	issn = {0360-0300},
	issue_date = {May 2023},
	journal = {ACM Comput. Surv.},
	keywords = {Algorithmic recourse, contrastive explanations and consequential recommendations},
	month = {dec},
	number = {5},
	numpages = {29},
	publisher = {Association for Computing Machinery},
	title = {A Survey of Algorithmic Recourse: Contrastive Explanations and Consequential Recommendations},
	url = {https://doi.org/10.1145/3527848},
	volume = {55},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1145/3527848}}

@article{guidotti2022counterfactual,
	abstract = {Interpretable machine learning aims at unveiling the reasons behind predictions returned by uninterpretable classifiers. One of the most valuable types of explanation consists of counterfactuals. A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome. For instance, a bank customer asks for a loan that is rejected. The counterfactual explanation consists of what should have been different for the customer in order to have the loan accepted. Recently, there has been an explosion of proposals for counterfactual explainers. The aim of this work is to survey the most recent explainers returning counterfactual explanations. We categorize explainers based on the approach adopted to return the counterfactuals, and we label them according to characteristics of the method and properties of the counterfactuals returned. In addition, we visually compare the explanations, and we report quantitative benchmarking assessing minimality, actionability, stability, diversity, discriminative power, and running time. The results make evident that the current state of the art does not provide a counterfactual explainer able to guarantee all these properties simultaneously.},
	author = {Guidotti, Riccardo},
	date = {2022-04-28},
	doi = {10.1007/s10618-022-00831-6},
	issn = {1573-756X},
	journal = {Data Mining and Knowledge Discovery},
	publisher = {Springer},
	title = {Counterfactual explanations and how to find them: literature review and benchmarking},
	url = {https://doi.org/10.1007/s10618-022-00831-6},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1007/s10618-022-00831-6}}

@article{sokol2022fat,
	author = {Sokol, Kacper and Santos-Rodriguez, Raul and Flach, Peter},
	journal = {Software Impacts},
	pages = {100406},
	publisher = {Elsevier},
	title = {{FAT Forensics}: {A} {Python} Toolbox for Algorithmic Fairness, Accountability and Transparency},
	year = {2022}}

@article{goethals2023precof,
	abstract = {This paper studies how counterfactual explanations can be used to assess the fairness of a model. Using machine learning for high-stakes decisions is a threat to fairness as these models can amplify bias present in the dataset, and there is no consensus on a universal metric to detect this. The appropriate metric and method to tackle the bias in a dataset will be case-dependent, and it requires insight into the nature of the bias first. We aim to provide this insight by integrating explainable AI (XAI) research with the fairness domain. More specifically, apart from being able to use (Predictive) Counterfactual Explanations to detect explicit bias when the model is directly using the sensitive attribute, we show that it can also be used to detect implicit bias when the model does not use the sensitive attribute directly but does use other correlated attributes leading to a substantial disadvantage for a protected group. We call this metric PreCoF, or Predictive Counterfactual Fairness. Our experimental results show that our metric succeeds in detecting occurrences of implicit bias in the model by assessing which attributes are more present in the explanations of the protected group compared to the unprotected group. These results could help policymakers decide on whether this discrimination is justified or not.},
	author = {Goethals, Sofie and Martens, David and Calders, Toon},
	date = {2023-03-28},
	doi = {10.1007/s10994-023-06319-8},
	issn = {1573-0565},
	journal = {Machine Learning},
	publisher = {Springer},
	title = {PreCoF: counterfactual explanations for fairness},
	url = {https://doi.org/10.1007/s10994-023-06319-8},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1007/s10994-023-06319-8}}

@misc{Dua_2019,
	author = {Dua, Dheeru and Graff, Casey},
	institution = {University of California, Irvine, School of Information and Computer Sciences},
	title = {{UCI} Machine Learning Repository},
	url = {http://archive.ics.uci.edu/ml},
	year = {2017},
	bdsk-url-1 = {http://archive.ics.uci.edu/ml}}

@book{Morik10,
	author = {K. Morik},
	booktitle = {Encyclopedia of Machine Learning},
	pages = {654--661},
	publisher = {Springer US},
	title = {Medicine: Applications of Machine Learning},
	year = {2010}}

@book{10.5555/573193,
	abstract = {From the Publisher:Neural networks are revolutionizing virtually every aspect of financial and investment decision making. Financial firms worldwide are employing neural networks to tackle difficult tasks involving intuitive judgement or requiring the detection of data patterns which elude conventional analytic techniques. Many observers believe neural networks will eventually outperform even the best traders and investors. Neural networks are already being used to trade the securities markets, to forecast the economy and to analyze credit risk. Indeed, apart from the U.S. Department of Defense, the financial services industry has invested more money in neural network research than any other industry or government body. Unlike other types of artificial intelligence, neural networks mimic to some extent the processing characteristics of the human brain. As a result, neural networks can draw conclusions from incomplete data, recognize patterns as they unfold in real time and forecast the future. They can even learn from past mistakes! In Neural Networks in Finance and Investing, Robert Trippi and Efraim Turban have assembled a stellar collection of articles by experts in industry and academia on the applications of neural networks in this important arena. They discuss neural network successes and failures, as well as identify the vast unrealized potential of neural networks in numerous specialized areas of financial decision making. Topics include neural network fundamentals and overview, analysis of financial condition, business failure prediction, debt risk assessment, security market applications, and neural network approaches to financial forecasting. Nowhere else will the finance professional find such an exciting and relevant in-depth examination of neural networks. Individual chapters discuss how to use neural networks to forecast the stock market, to trade commodities, to assess bond and mortgage risk, to predict bankruptcy and to implement investment strategies. Taken toge},
	address = {USA},
	author = {Trippi, Robert R. and Turban, Efraim},
	isbn = {1557384525},
	publisher = {McGraw-Hill, Inc.},
	title = {Neural Networks in Finance and Investing: Using Artificial Intelligence to Improve Real World Performance},
	year = {1992}}

@article{angwin2016machine,
	added-at = {2020-09-14T11:52:21.000+0200},
	author = {Angwin, Julia and Larson, Jeff and Mattu, Surya and Kirchner, Lauren},
	biburl = {https://www.bibsonomy.org/bibtex/23d537ed0185eb7820ed6769aca10acb4/wanlo},
	interhash = {e9260c2f8fdd08ef1a34f7a3a243b0ff},
	intrahash = {3d537ed0185eb7820ed6769aca10acb4},
	journaltitle = {Propublica},
	keywords = {background},
	timestamp = {2020-09-14T11:52:21.000+0200},
	title = {{Machine Bias}},
	url = {https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing},
	year = {2016},
	bdsk-url-1 = {https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing}}

@article{DBLP:journals/corr/abs-1905-12728,
	author = {Fernando Mart{\'{\i}}nez{-}Plumed and C{\`{e}}sar Ferri and David Nieves and Jos{\'{e}} Hern{\'{a}}ndez{-}Orallo},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/corr/abs-1905-12728.bib},
	eprint = {1905.12728},
	eprinttype = {arXiv},
	journal = {CoRR},
	timestamp = {Thu, 14 Oct 2021 09:16:55 +0200},
	title = {Fairness and Missing Values},
	url = {http://arxiv.org/abs/1905.12728},
	volume = {abs/1905.12728},
	year = {2019},
	bdsk-url-1 = {http://arxiv.org/abs/1905.12728}}

@online{ProPublica2,
	author = {Julia Angwin and Jeff Larson and Mattu Surya and Lauren Kirchner},
	title = {How we analysed the COMPAS recidivism algorithm. \textit{ProPublica}.},
	url = {https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm},
	urldate = {2016-05-23},
	year = 2016,
	bdsk-url-1 = {https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm}}

@inproceedings{pmlr-v81-buolamwini18a,
	author = {Buolamwini, Joy and Gebru, Timnit},
	booktitle = {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
	editor = {Friedler, Sorelle A. and Wilson, Christo},
	month = {23--24 Feb},
	pages = {77--91},
	pdf = {http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf},
	publisher = {PMLR},
	series = {Proceedings of Machine Learning Research},
	title = {Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification},
	url = {https://proceedings.mlr.press/v81/buolamwini18a.html},
	volume = {81},
	year = {2018},
	bdsk-url-1 = {https://proceedings.mlr.press/v81/buolamwini18a.html}}

@inproceedings{10.1145/2702123.2702520,
	address = {New York, NY, USA},
	author = {Kay, Matthew and Matuszek, Cynthia and Munson, Sean A.},
	booktitle = {Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems},
	doi = {10.1145/2702123.2702520},
	isbn = {9781450331456},
	location = {Seoul, Republic of Korea},
	numpages = {10},
	pages = {3819--3828},
	publisher = {Association for Computing Machinery},
	series = {CHI '15},
	title = {Unequal Representation and Gender Stereotypes in Image Search Results for Occupations},
	url = {https://doi.org/10.1145/2702123.2702520},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1145/2702123.2702520}}

@article{Leicht-Deobald,
	author = {Leicht-Deobald, Ulrich and Busch, Thorsten and Schank, Christoph and Weibel, Antoinette and Schafheitle, Simon and Wildhaber, Isabelle and Kasper, Gabriel},
	doi = {10.1007/s10551-019-04204-w},
	journal = {Journal of Business Ethics},
	month = {12},
	pages = {377--392},
	title = {The Challenges of Algorithm-Based HR Decision-Making for Personal Integrity},
	volume = {160(2)},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1007/s10551-019-04204-w}}

@article{act1964civil,
	author = {{An Act}},
	journal = {Title VII, Equal Employment Opportunities},
	title = {Civil Rights Act},
	year = {1964}}

@misc{chouldechova2018frontiers,
	archiveprefix = {arXiv},
	author = {Alexandra Chouldechova and Aaron Roth},
	eprint = {1810.08810},
	primaryclass = {cs.LG},
	title = {The Frontiers of Fairness in Machine Learning},
	year = {2018}}

@article{wang2022brief,
	abstract = {Machine learning algorithms are widely used in management systems in different fields, such as employee recruitment, loan provision, disease diagnosis, etc., and even in some risky decision-making areas, playing an increasingly crucial role in decisions affecting people's lives and social development. However, the use of algorithms for automated decision-making can cause unintentional biases that lead to discrimination against certain specific groups. In this context, it is crucial to develop machine learning algorithms that are not only accurate but also fair. There is an extensive discussion of algorithmic fairness in the existing literature. Many scholars have proposed and tested definitions of fairness and attempted to address the problem of unfairness or discrimination in algorithms. This review aims to outline different definitions of algorithmic fairness and to introduce the procedure for constructing fair algorithms to enhance fairness in machine learning. First, this review divides the definitions of algorithmic fairness into two categories, namely, awareness-based fairness and rationality-based fairness, and discusses existing representative algorithmic fairness concepts and notions based on the two categories. Then, metrics for unfairness/discrimination identification are summarized and different unfairness/discrimination removal approaches are discussed to facilitate a better understanding of how algorithmic fairness can be implemented in different scenarios. Challenges and future research directions in the field of algorithmic fairness are finally concluded.},
	author = {Wang, Xiaomeng and Zhang, Yishi and Zhu, Ruilin},
	date = {2022-11-10},
	doi = {10.1007/s44176-022-00006-z},
	issn = {2731-5843},
	journal = {Management System Engineering},
	number = {1},
	pages = {7},
	publisher = {Springer},
	title = {A brief review on algorithmic fairness},
	url = {https://doi.org/10.1007/s44176-022-00006-z},
	volume = {1},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1007/s44176-022-00006-z}}

@inproceedings{Barocas2018FairnessAM,
	author = {Solon Barocas and Moritz Hardt and Arvind Narayanan},
	publisher = {MIT Press},
	title = {Fairness and Machine Learning Limitations and Opportunities},
	url = {https://fairmlbook.org/},
	year = {2018},
	bdsk-url-1 = {https://fairmlbook.org/}}

@article{Liu_trade,
	author = {Liu, Suyun and Vicente, Luis},
	doi = {10.1007/s10287-022-00425-z},
	journal = {Computational Management Science},
	month = {07},
	title = {Accuracy and fairness trade-offs in machine learning: a stochastic multi-objective approach},
	volume = {19},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1007/s10287-022-00425-z}}

@inproceedings{pmlr-v80-agarwal18a,
	author = {Agarwal, Alekh and Beygelzimer, Alina and Dudik, Miroslav and Langford, John and Wallach, Hanna},
	booktitle = {Proceedings of the 35th International Conference on Machine Learning},
	editor = {Dy, Jennifer and Krause, Andreas},
	month = {10--15 Jul},
	pages = {60--69},
	pdf = {http://proceedings.mlr.press/v80/agarwal18a/agarwal18a.pdf},
	publisher = {PMLR},
	series = {Proceedings of Machine Learning Research},
	title = {A Reductions Approach to Fair Classification},
	url = {https://proceedings.mlr.press/v80/agarwal18a.html},
	volume = {80},
	year = {2018},
	bdsk-url-1 = {https://proceedings.mlr.press/v80/agarwal18a.html}}

@article{bechavod2018penalizing,
	author = {Bechavod, Yahav and Ligett, Katrina},
	month = {06},
	pages = {1733--1782},
	title = {Learning Fair Classifiers: A Regularization-Inspired Approach},
	year = {2017}}

@inproceedings{10.1007/978-3-642-33486-3_3,
	address = {Berlin, Heidelberg},
	author = {Kamishima, Toshihiro and Akaho, Shotaro and Asoh, Hideki and Sakuma, Jun},
	booktitle = {Machine Learning and Knowledge Discovery in Databases},
	editor = {Flach, Peter A. and De Bie, Tijl and Cristianini, Nello},
	pages = {35--50},
	publisher = {Springer Berlin Heidelberg},
	title = {Fairness-Aware Classifier with Prejudice Remover Regularizer},
	year = {2012}}

@inproceedings{10.1145/3038912.3052660,
	address = {Republic and Canton of Geneva, CHE},
	author = {Zafar, Muhammad Bilal and Valera, Isabel and Gomez Rodriguez, Manuel and Gummadi, Krishna P.},
	booktitle = {Proceedings of the 26th International Conference on World Wide Web},
	doi = {10.1145/3038912.3052660},
	isbn = {9781450349130},
	location = {Perth, Australia},
	numpages = {10},
	pages = {1171--1180},
	publisher = {International World Wide Web Conferences Steering Committee},
	series = {WWW '17},
	title = {Fairness Beyond Disparate Treatment \& Disparate Impact: Learning Classification without Disparate Mistreatment},
	url = {https://doi.org/10.1145/3038912.3052660},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1145/3038912.3052660}}

@inproceedings{pmlr-v54-zafar17a,
	author = {Zafar, Muhammad Bilal and Valera, Isabel and Rogriguez, Manuel Gomez and Gummadi, Krishna P.},
	booktitle = {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics},
	editor = {Singh, Aarti and Zhu, Jerry},
	month = {20--22 Apr},
	pages = {962--970},
	pdf = {http://proceedings.mlr.press/v54/zafar17a/zafar17a.pdf},
	publisher = {PMLR},
	series = {Proceedings of Machine Learning Research},
	title = {{Fairness Constraints: Mechanisms for Fair Classification}},
	url = {https://proceedings.mlr.press/v54/zafar17a.html},
	volume = {54},
	year = {2017},
	bdsk-url-1 = {https://proceedings.mlr.press/v54/zafar17a.html}}

@inproceedings{NIPS2016_9d268236,
	author = {Hardt, Moritz and Price, Eric and Price, Eric and Srebro, Nati},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {Equality of Opportunity in Supervised Learning},
	url = {https://proceedings.neurips.cc/paper_files/paper/2016/file/9d2682367c3935defcb1f9e247a97c0d-Paper.pdf},
	volume = {29},
	year = {2016},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2016/file/9d2682367c3935defcb1f9e247a97c0d-Paper.pdf}}

@article{10.2307/24758720,
	author = {Solon Barocas and Andrew D. Selbst},
	issn = {00081221},
	journal = {California Law Review},
	number = {3},
	pages = {671--732},
	publisher = {California Law Review, Inc.},
	title = {Big Data's Disparate Impact},
	url = {http://www.jstor.org/stable/24758720},
	urldate = {2023-04-24},
	volume = {104},
	year = {2016},
	bdsk-url-1 = {http://www.jstor.org/stable/24758720}}

@inproceedings{pmlr-v81-dwork18a,
	author = {Dwork, Cynthia and Immorlica, Nicole and Kalai, Adam Tauman and Leiserson, Max},
	booktitle = {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
	editor = {Friedler, Sorelle A. and Wilson, Christo},
	month = {23--24 Feb},
	pages = {119--133},
	pdf = {http://proceedings.mlr.press/v81/dwork18a/dwork18a.pdf},
	publisher = {PMLR},
	series = {Proceedings of Machine Learning Research},
	title = {Decoupled Classifiers for Group-Fair and Efficient Machine Learning},
	url = {https://proceedings.mlr.press/v81/dwork18a.html},
	volume = {81},
	year = {2018},
	bdsk-url-1 = {https://proceedings.mlr.press/v81/dwork18a.html}}

@misc{EU-GDPR-2016,
	author = {{European Union}},
	howpublished = {\url{http://data.europa.eu/eli/reg/2016/679/2016-05-04}},
	note = {Official Journal of the European Union},
	title = {{EU General Data Protection Regulation (GDPR): Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC (General Data Protection Regulation)}},
	year = {2016}}

@article{wachter2017counterfactual,
	author = {Wachter, Sandra and Mittelstadt, Brent and Russell, Chris},
	journal = {Harvard Journal of Law \& Technology (Harvard JOLT)},
	language = {English},
	number = {2},
	pages = {841--888},
	publisher = {Harvard Journal of Law \& Technology},
	title = {Counterfactual Explanations without Opening the Black Box: Automated Decisions and the GDPR},
	url = {https://heinonline.org/HOL/P?h=hein.journals/hjlt31&i=859},
	urldate = {2024-02-17},
	volume = {31},
	year = {2017},
	bdsk-url-1 = {https://heinonline.org/HOL/P?h=hein.journals/hjlt31&i=859}}

@inproceedings{10.1145/3351095.3372864,
	abstract = {A distinction has been drawn in fair machine learning research between 'group' and 'individual' fairness measures. Many technical research papers assume that both are important, but conflicting, and propose ways to minimise the trade-offs between these measures. This paper argues that this apparent conflict is based on a misconception. It draws on discussions from within the fair machine learning research, and from political and legal philosophy, to argue that individual and group fairness are not fundamentally in conflict. First, it outlines accounts of egalitarian fairness which encompass plausible motivations for both group and individual fairness, thereby suggesting that there need be no conflict in principle. Second, it considers the concept of individual justice, from legal philosophy and jurisprudence, which seems similar but actually contradicts the notion of individual fairness as proposed in the fair machine learning literature. The conclusion is that the apparent conflict between individual and group fairness is more of an artefact of the blunt application of fairness measures, rather than a matter of conflicting principles. In practice, this conflict may be resolved by a nuanced consideration of the sources of 'unfairness' in a particular deployment context, and the carefully justified application of measures to mitigate it.},
	address = {New York, NY, USA},
	author = {Binns, Reuben},
	booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
	doi = {10.1145/3351095.3372864},
	isbn = {9781450369367},
	keywords = {statistical parity, machine learning, justice, individual fairness, fairness, discrimination},
	location = {Barcelona, Spain},
	numpages = {11},
	pages = {514--524},
	publisher = {Association for Computing Machinery},
	series = {FAT* '20},
	title = {On the apparent conflict between individual and group fairness},
	url = {https://doi.org/10.1145/3351095.3372864},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1145/3351095.3372864}}

@inproceedings{10.1145/2090236.2090255,
	abstract = {We study fairness in classification, where individuals are classified, e.g., admitted to a university, and the goal is to prevent discrimination against individuals based on their membership in some group, while maintaining utility for the classifier (the university). The main conceptual contribution of this paper is a framework for fair classification comprising (1) a (hypothetical) task-specific metric for determining the degree to which individuals are similar with respect to the classification task at hand; (2) an algorithm for maximizing utility subject to the fairness constraint, that similar individuals are treated similarly. We also present an adaptation of our approach to achieve the complementary goal of "fair affirmative action," which guarantees statistical parity (i.e., the demographics of the set of individuals receiving any classification are the same as the demographics of the underlying population), while treating similar individuals as similarly as possible. Finally, we discuss the relationship of fairness to privacy: when fairness implies privacy, and how tools developed in the context of differential privacy may be applied to fairness.},
	address = {New York, NY, USA},
	author = {Dwork, Cynthia and Hardt, Moritz and Pitassi, Toniann and Reingold, Omer and Zemel, Richard},
	booktitle = {Proceedings of the 3rd Innovations in Theoretical Computer Science Conference},
	doi = {10.1145/2090236.2090255},
	isbn = {9781450311151},
	location = {Cambridge, Massachusetts},
	numpages = {13},
	pages = {214--226},
	publisher = {Association for Computing Machinery},
	series = {ITCS '12},
	title = {Fairness through awareness},
	url = {https://doi.org/10.1145/2090236.2090255},
	year = {2012},
	bdsk-url-1 = {https://doi.org/10.1145/2090236.2090255}}

@article{10.1145/3236009,
	abstract = {In recent years, many accurate decision support systems have been constructed as black boxes, that is as systems that hide their internal logic to the user. This lack of explanation constitutes both a practical and an ethical issue. The literature reports many approaches aimed at overcoming this crucial weakness, sometimes at the cost of sacrificing accuracy for interpretability. The applications in which black box decision systems can be used are various, and each approach is typically developed to provide a solution for a specific problem and, as a consequence, it explicitly or implicitly delineates its own definition of interpretability and explanation. The aim of this article is to provide a classification of the main problems addressed in the literature with respect to the notion of explanation and the type of black box system. Given a problem definition, a black box type, and a desired explanation, this survey should help the researcher to find the proposals more useful for his own work. The proposed classification of approaches to open black box models should also be useful for putting the many research open questions in perspective.},
	address = {New York, NY, USA},
	articleno = {93},
	author = {Guidotti, Riccardo and Monreale, Anna and Ruggieri, Salvatore and Turini, Franco and Giannotti, Fosca and Pedreschi, Dino},
	doi = {10.1145/3236009},
	issn = {0360-0300},
	issue_date = {September 2019},
	journal = {ACM Comput. Surv.},
	keywords = {Open the black box, explanations, interpretability, transparent models},
	month = {aug},
	number = {5},
	numpages = {42},
	publisher = {Association for Computing Machinery},
	title = {A Survey of Methods for Explaining Black Box Models},
	url = {https://doi.org/10.1145/3236009},
	volume = {51},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1145/3236009}}

@book{byrne2007rational,
	author = {Byrne, Ruth M.J.},
	publisher = {MIT Press},
	title = {The Rational Imagination: How People Create Alternatives to Reality},
	year = {2007}}

@article{verma2020counterfactual,
	archiveprefix = {arXiv},
	author = {Verma, Sahil and Dickerson, John and Hines, Kathryn},
	eprint = {2010.10596},
	primaryclass = {cs.LG},
	title = {Counterfactual explanations for machine learning: A review},
	year = {2020}}

@article{10.1145/3494672,
	abstract = {An increasing number of decisions regarding the daily lives of human beings are being controlled by artificial intelligence and machine learning (ML) algorithms in spheres ranging from healthcare, transportation, and education to college admissions, recruitment, provision of loans, and many more realms. Since they now touch on many aspects of our lives, it is crucial to develop ML algorithms that are not only accurate but also objective and fair. Recent studies have shown that algorithmic decision making may be inherently prone to unfairness, even when there is no intention for it. This article presents an overview of the main concepts of identifying, measuring, and improving algorithmic fairness when using ML algorithms, focusing primarily on classification tasks. The article begins by discussing the causes of algorithmic bias and unfairness and the common definitions and measures for fairness. Fairness-enhancing mechanisms are then reviewed and divided into pre-process, in-process, and post-process mechanisms. A comprehensive comparison of the mechanisms is then conducted, toward a better understanding of which mechanisms should be used in different scenarios. The article ends by reviewing several emerging research sub-fields of algorithmic fairness, beyond classification.},
	address = {New York, NY, USA},
	articleno = {51},
	author = {Pessach, Dana and Shmueli, Erez},
	doi = {10.1145/3494672},
	issn = {0360-0300},
	issue_date = {March 2023},
	journal = {ACM Comput. Surv.},
	keywords = {fairness in machine learning, fairness-aware machine learning, algorithmic fairness, Algorithmic bias},
	month = {feb},
	number = {3},
	numpages = {44},
	publisher = {Association for Computing Machinery},
	title = {A Review on Fairness in Machine Learning},
	url = {https://doi.org/10.1145/3494672},
	volume = {55},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1145/3494672}}

@article{9321372,
	abstract = {A number of algorithms in the field of artificial intelligence offer poorly interpretable decisions. To disclose the reasoning behind such algorithms, their output can be explained by means of so-called evidence-based (or factual) explanations. Alternatively, contrastive and counterfactual explanations justify why the output of the algorithms is not any different and how it could be changed, respectively. It is of crucial importance to bridge the gap between theoretical approaches to contrastive and counterfactual explanation and the corresponding computational frameworks. In this work we conduct a systematic literature review which provides readers with a thorough and reproducible analysis of the interdisciplinary research field under study. We first examine theoretical foundations of contrastive and counterfactual accounts of explanation. Then, we report the state-of-the-art computational frameworks for contrastive and counterfactual explanation generation. In addition, we analyze how grounded such frameworks are on the insights from the inspected theoretical approaches. As a result, we highlight a variety of properties of the approaches under study and reveal a number of shortcomings thereof. Moreover, we define a taxonomy regarding both theoretical and practical approaches to contrastive and counterfactual explanation.},
	author = {Stepin, Ilia and Alonso, Jose M. and Catala, Alejandro and Pereira-Fari{\~n}a, Mart{\'\i}n},
	doi = {10.1109/ACCESS.2021.3051315},
	issn = {2169-3536},
	journal = {IEEE Access},
	keywords = {Cognition;Artificial intelligence;Training;Terminology;Taxonomy;Systematics;Signal to noise ratio;Computational intelligence;contrastive explanations;counterfactuals;explainable artificial intelligence;systematic literature review},
	pages = {11974-12001},
	title = {A Survey of Contrastive and Counterfactual Explanation Generation Methods for Explainable Artificial Intelligence},
	volume = {9},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/ACCESS.2021.3051315}}

@article{artelt2019computation,
	author = {Artelt, Andr{\'e} and Hammer, Barbara},
	journal = {arXiv preprint arXiv:1911.07749},
	title = {On the computation of counterfactual explanations--A survey},
	year = {2019}}

@misc{karimi2021survey,
	archiveprefix = {arXiv},
	author = {Amir-Hossein Karimi and Gilles Barthe and Bernhard Sch{\"o}lkopf and Isabel Valera},
	eprint = {2010.04050},
	primaryclass = {cs.LG},
	title = {A survey of algorithmic recourse: definitions, formulations, solutions, and prospects},
	year = {2021}}

@inproceedings{ijcai2019p199,
	author = {Wu, Yongkai and Zhang, Lu and Wu, Xintao},
	booktitle = {Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, {IJCAI-19}},
	doi = {10.24963/ijcai.2019/199},
	month = {7},
	pages = {1438--1444},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	title = {Counterfactual Fairness: Unidentification, Bound and Algorithm},
	url = {https://doi.org/10.24963/ijcai.2019/199},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.24963/ijcai.2019/199}}

@article{Zhang_Bareinboim_2018,
	abstractnote = {&lt;p&gt; AI plays an increasingly prominent role in society since decisions that were once made by humans are now delegated to automated systems. These systems are currently in charge of deciding bank loans, criminals' incarceration, and the hiring of new employees, and it's not difficult to envision that they will in the future underpin most of the decisions in society. Despite the high complexity entailed by this task, there is still not much understanding of basic properties of such systems. For instance, we currently cannot detect (neither explain nor correct) whether an AI system can be deemed fair (i.e., is abiding by the decision-constraints agreed by society) or it is reinforcing biases and perpetuating a preceding prejudicial practice. Issues of discrimination have been discussed extensively in political and legal circles, but there exists still not much understanding of the formal conditions that a system must meet to be deemed fair. In this paper, we use the language of structural causality (Pearl, 2000) to fill in this gap. We start by introducing three new fine-grained measures of transmission of change from stimulus to effect, which we called counterfactual direct (Ctf-DE), indirect (Ctf-IE), and spurious (Ctf-SE) effects. We then derive what we call the causal explanation formula, which allows the AI designer to quantitatively evaluate fairness and explain the total observed disparity of decisions through different discriminatory mechanisms. We apply these measures to various discrimination analysis tasks and run extensive simulations, including detection, evaluation, and optimization of decision-making under fairness constraints. We conclude studying the trade-off between different types of fairness criteria (outcome and procedural), and provide a quantitative approach to policy implementation and the design of fair AI systems. &lt;/p&gt;},
	author = {Zhang, Junzhe and Bareinboim, Elias},
	doi = {10.1609/aaai.v32i1.11564},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	month = {Apr.},
	number = {1},
	title = {Fairness in Decision-Making --- The Causal Explanation Formula},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/11564},
	volume = {32},
	year = {2018},
	bdsk-url-1 = {https://ojs.aaai.org/index.php/AAAI/article/view/11564},
	bdsk-url-2 = {https://doi.org/10.1609/aaai.v32i1.11564}}

@article{Chiappa_2019,
	abstractnote = {&lt;p&gt;We consider the problem of learning fair decision systems from data in which a sensitive attribute might affect the decision along both fair and unfair pathways. We introduce a counterfactual approach to disregard effects along unfair pathways that does not incur in the same loss of individual-specific information as previous approaches. Our method corrects observations adversely affected by the sensitive attribute, and uses these to form a decision. We leverage recent developments in deep learning and approximate inference to develop a VAE-type method that is widely applicable to complex nonlinear models.&lt;/p&gt;},
	author = {Chiappa, Silvia},
	doi = {10.1609/aaai.v33i01.33017801},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	month = {Jul.},
	number = {01},
	pages = {7801-7808},
	title = {Path-Specific Counterfactual Fairness},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/4777},
	volume = {33},
	year = {2019},
	bdsk-url-1 = {https://ojs.aaai.org/index.php/AAAI/article/view/4777},
	bdsk-url-2 = {https://doi.org/10.1609/aaai.v33i01.33017801}}

@inproceedings{pan2021explaining,
	author = {Pan, Wei and Cui, Shiyang and Bian, Jiang and Zhang, Cheng and Wang, Fei},
	booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \& Data Mining},
	pages = {1287--1297},
	title = {Explaining algorithmic fairness through fairness-aware causal path decomposition},
	year = {2021}}

@inproceedings{10.1609/aaai.v37i9.26344,
	abstract = {While machine learning models have achieved unprecedented success in real-world applications, they might make biased/unfair decisions for specific demographic groups and hence result in discriminative outcomes. Although research efforts have been devoted to measuring and mitigating bias, they mainly study bias from the result-oriented perspective while neglecting the bias encoded in the decision-making procedure. This results in their inability to capture procedure-oriented bias, which therefore limits the ability to have a fully debiasing method. Fortunately, with the rapid development of explainable machine learning, explanations for predictions are now available to gain insights into the procedure. In this work, we bridge the gap between fairness and explainability by presenting a novel perspective of procedure-oriented fairness based on explanations. We identify the procedure-based bias by measuring the gap of explanation quality between different groups with Ratio-based and Value-based Explanation Fairness. The new metrics further motivate us to design an optimization objective to mitigate the procedure-based bias where we observe that it will also mitigate bias from the prediction. Based on our designed optimization objective, we propose a Comprehensive Fairness Algorithm (CFA), which simultaneously fulfills multiple objectives - improving traditional fairness, satisfying explanation fairness, and maintaining the utility performance. Extensive experiments on real-world datasets demonstrate the effectiveness of our proposed CFA and highlight the importance of considering fairness from the explainability perspective.},
	articleno = {1275},
	author = {Zhao, Yuying and Wang, Yu and Derr, Tyler},
	booktitle = {Proceedings of the Thirty-Seventh AAAI Conference on Artificial Intelligence and Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence and Thirteenth Symposium on Educational Advances in Artificial Intelligence},
	doi = {10.1609/aaai.v37i9.26344},
	isbn = {978-1-57735-880-0},
	numpages = {9},
	publisher = {AAAI Press},
	series = {AAAI'23/IAAI'23/EAAI'23},
	title = {Fairness and explainability: bridging the gap towards fair model explanations},
	url = {https://doi.org/10.1609/aaai.v37i9.26344},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1609/aaai.v37i9.26344}}

@online{loandataset,
	author = {Burak Erg{\"u}n},
	title = {Predict Loan Eligibility for Dream Housing Finance company},
	url = {https://datahack.analyticsvidhya.com/contest/practice-problem-loan-prediction-iii/},
	urldate = {2016-05-26},
	bdsk-url-1 = {https://datahack.analyticsvidhya.com/contest/practice-problem-loan-prediction-iii/}}

@misc{europeancommission2021,
	author = {{European Commission}},
	howpublished = {https://digital-strategy.ec.europa.eu/en/library/proposal-regulationlaying-down-harmonised-rules-artificial-intelligence},
	title = {{Proposal for a Regulation of the European Parliament and of the Council laying down harmonised rules on Artificial Intelligence (Artificial Intelligence Act) and amending certain Union legislative acts}},
	year = {2021}}

@article{mucsanyi2023trustworthy,
	author = {Mucs{\'a}nyi, B{\'a}lint and Kirchhof, Michael and Nguyen, Elisa and Rubinstein, Alexander and Oh, Seong Joon},
	journal = {arXiv preprint arXiv:2310.08215},
	title = {Trustworthy Machine Learning},
	year = {2023}}

@article{8466590,
	abstract = {At the dawn of the fourth industrial revolution, we are witnessing a fast and widespread adoption of artificial intelligence (AI) in our daily life, which contributes to accelerating the shift towards a more algorithmic society. However, even with such unprecedented advancements, a key impediment to the use of AI-based systems is that they often lack transparency. Indeed, the black-box nature of these systems allows powerful predictions, but it cannot be directly explained. This issue has triggered a new debate on explainable AI (XAI). A research field holds substantial promise for improving trust and transparency of AI-based systems. It is recognized as the sine qua non for AI to continue making steady progress without disruption. This survey provides an entry point for interested researchers and practitioners to learn key aspects of the young and rapidly growing body of research related to XAI. Through the lens of the literature, we review the existing approaches regarding the topic, discuss trends surrounding its sphere, and present major research trajectories.},
	author = {Adadi, Amina and Berrada, Mohammed},
	doi = {10.1109/ACCESS.2018.2870052},
	issn = {2169-3536},
	journal = {IEEE Access},
	keywords = {Conferences;Machine learning;Market research;Prediction algorithms;Machine learning algorithms;Biological system modeling;Explainable artificial intelligence;interpretable machine learning;black-box models},
	pages = {52138-52160},
	title = {Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence (XAI)},
	volume = {6},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/ACCESS.2018.2870052}}

@article{10.1145/3616865,
	abstract = {When Machine Learning technologies are used in contexts that affect citizens, companies as well as researchers need to be confident that there will not be any unexpected social implications, such as bias towards gender, ethnicity, and/or people with disabilities. There is significant literature on approaches to mitigate bias and promote fairness, yet the area is complex and hard to penetrate for newcomers to the domain. This article seeks to provide an overview of the different schools of thought and approaches that aim to increase the fairness of Machine Learning. It organises approaches into the widely accepted framework of pre-processing, in-processing, and post-processing methods, subcategorizing into a further 11 method areas. Although much of the literature emphasizes binary classification, a discussion of fairness in regression, recommender systems, and unsupervised learning is also provided along with a selection of currently available open source libraries. The article concludes by summarising open challenges articulated as five dilemmas for fairness research.},
	address = {New York, NY, USA},
	author = {Caton, Simon and Haas, Christian},
	doi = {10.1145/3616865},
	issn = {0360-0300},
	journal = {ACM Comput. Surv.},
	keywords = {transparency, machine learning, fairness, accountability},
	month = {aug},
	note = {Just Accepted},
	publisher = {Association for Computing Machinery},
	title = {Fairness in Machine Learning: A Survey},
	url = {https://doi.org/10.1145/3616865},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1145/3616865}}

@misc{corbettdavies2023measure,
	archiveprefix = {arXiv},
	author = {Sam Corbett-Davies and Johann D. Gaebler and Hamed Nilforoshan and Ravi Shroff and Sharad Goel},
	eprint = {1808.00023},
	primaryclass = {cs.CY},
	title = {The Measure and Mismeasure of Fairness},
	year = {2023}}

@article{psyke-ia2022,
	author = {Sabbatini, Federico and Ciatto, Giovanni and Calegari, Roberta and Omicini, Andrea},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/ia/SabbatiniCCO22.bib},
	doi = {10.3233/IA-210120},
	journal = {Intelligenza Artificiale},
	number = {1},
	pages = {27--48},
	timestamp = {Sun, 24 Jul 2022 12:56:47 +0200},
	title = {Symbolic Knowledge Extraction from Opaque {ML} Predictors in {PSyKE}: Platform Design {\&} Experiments},
	url = {https://doi.org/10.3233/IA-210120},
	volume = {16},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.3233/IA-210120}}

@inproceedings{gridrex-kr2022,
	author = {Sabbatini, Federico and Calegari, Roberta},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/kr/SabbatiniC22.bib},
	booktitle = {Proceedings of the 19th International Conference on Principles of Knowledge Representation and Reasoning, {KR} 2022, Haifa, Israel. July 31 - August 5, 2022},
	doi = {10.24963/kr.2022/57},
	editor = {Kern{-}Isberner, Gabriele and Lakemeyer, Gerhard and Meyer, Thomas},
	timestamp = {Wed, 27 Jul 2022 16:12:10 +0200},
	title = {Symbolic Knowledge Extraction from Opaque Machine Learning Predictors: {GridREx} {\&} {PEDRO}},
	url = {https://proceedings.kr.org/2022/57/},
	year = {2022},
	bdsk-url-1 = {https://proceedings.kr.org/2022/57/}}

@incollection{gridex-extraamas2021,
	_url = {http://link.springer.com/10.1007/978-3-030-82017-6_2},
	address = {Basel, Switzerland},
	author = {Sabbatini, Federico and Ciatto, Giovanni and Omicini, Andrea},
	booktitle = {Explainable and Transparent AI and Multi-Agent Systems. Third International Workshop, EXTRAAMAS 2021, Virtual Event, May 3--7, 2021, Revised Selected Papers},
	date-modified = {2021-09-17 17:29:10 +0200},
	doi = {10.1007/978-3-030-82017-6_2},
	editor = {Calvaresi, Davide and Najjar, Amro and Winikoff, Michael and Fr{\"a}mling, Kary},
	isbn = {978-3-030-82016-9},
	isbn-online = {978-3-030-82017-6},
	issn = {0302-9743},
	issn-online = {1611-3349},
	pages = {18--38},
	publisher = {Springer Nature},
	series = {LNCS},
	subseries = {Lecture Notes in Artificial Intelligence},
	title = {{GridEx}: An Algorithm for Knowledge Extraction from Black-Box Regressors},
	volume = 12688,
	year = 2021,
	bdsk-url-1 = {https://doi.org/10.1007/978-3-030-82017-6_2}}

@article{psyke-trust-aixia2022,
	address = {Cham, Switzerland},
	author = {Calegari, Roberta and Sabbatini, Federico},
	booktitle = {AIxIA 2022},
	doi = {10.1007/978-3-031-27181-6_1},
	editor = {Dovier, Agostino and Montanari, Angelo and Orlandini, Andrea},
	eisbn = {978-3-031-27181-6},
	institution = {University of Udine},
	isbn = {978-3-031-27180-9},
	issn = {0302-9743},
	location = {Udine, Italy},
	month = mar,
	note = {XXI International Conference of the Italian Association for Artificial Intelligence, AIxIA 2022, Udine, Italy, November 28 -- December 2, 2022, Proceedings},
	numpages = 14,
	pages = {3--16},
	publisher = {Springer},
	series = {Lecture Notes in Computer Science},
	subseries = {Lecture Notes in Artificial Intelligence},
	title = {The {PSyKE} Technology for Trustworthy Artificial Intelligence},
	url = {https://doi.org/10.1007/978-3-031-27181-6_1},
	volume = 13796,
	year = 2023,
	bdsk-url-1 = {https://doi.org/10.1007/978-3-031-27181-6_1}}

@article{SKESKISLR2024,
	author = {Ciatto, Giovanni and Sabbatini, Federico and Agiollo, Andrea and Magnini, Matteo and Omicini, Andrea},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/csur/CiattoSAMO24.bib},
	doi = {10.1145/3645103},
	journal = {{ACM} Computing Surveys},
	number = {6},
	pages = {161:1--161:35},
	timestamp = {Mon, 15 Apr 2024 08:25:48 +0200},
	title = {Symbolic Knowledge Extraction and Injection with Sub-symbolic Predictors: {A} Systematic Literature Review},
	url = {https://doi.org/10.1145/3645103},
	volume = {56},
	year = {2024},
	bdsk-url-1 = {https://doi.org/10.1145/3645103}}

@book{Wiggins2020calculating,
	author = {Wiggins, Benjamin},
	doi = {10.1093/oso/9780197504000.001.0001},
	isbn = {9780197504000},
	month = {11},
	publisher = {Oxford University Press},
	title = {Calculating Race: Racial Discrimination in Risk Assessment},
	url = {https://doi.org/10.1093/oso/9780197504000.001.0001},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1093/oso/9780197504000.001.0001}}

@article{NuenenASC20,
	author = {van Nuenen, Tom and Ferrer, Xavier and Such, Jose M. and Cot{\'{e}}, Mark},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/computer/NuenenASC20.bib},
	doi = {10.1109/MC.2020.3002181},
	journal = {Computer},
	number = {11},
	pages = {36--44},
	timestamp = {Tue, 29 Dec 2020 18:15:24 +0100},
	title = {Transparency for Whom? Assessing Discriminatory Artificial Intelligence},
	url = {https://doi.org/10.1109/MC.2020.3002181},
	volume = {53},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/MC.2020.3002181}}

@article{QuyRIZN22,
	author = {Quy, Tai Le and Roy, Arjun and Iosifidis, Vasileios and Zhang, Wenbin and Ntoutsi, Eirini},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/widm/QuyRIZN22.bib},
	doi = {10.1002/WIDM.1452},
	journal = {WIREs Data Mining and Knowledge Discovery},
	number = {3},
	timestamp = {Tue, 12 Sep 2023 07:58:39 +0200},
	title = {A survey on datasets for fairness-aware machine learning},
	url = {https://doi.org/10.1002/widm.1452},
	volume = {12},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1002/widm.1452}}

@inproceedings{ChakrabortyPM20,
	author = {Chakraborty, Joymallya and Peng, Kewen and Menzies, Tim},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/kbse/ChakrabortyPM20.bib},
	booktitle = {35th {IEEE/ACM} International Conference on Automated Software Engineering, {ASE} 2020, Melbourne, Australia, September 21--25, 2020},
	doi = {10.1145/3324884.3418932},
	pages = {1229--1233},
	publisher = {{IEEE}},
	timestamp = {Mon, 05 Feb 2024 20:31:35 +0100},
	title = {Making Fair {ML} Software using Trustworthy Explanation},
	url = {https://doi.org/10.1145/3324884.3418932},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1145/3324884.3418932}}

@inproceedings{FeldmanFMSV15,
	author = {Feldman, Michael and Friedler, Sorelle A. and Moeller, John and Scheidegger, Carlos and Venkatasubramanian, Suresh},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/kdd/FeldmanFMSV15.bib},
	booktitle = {Proceedings of the 21th {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data Mining, Sydney, NSW, Australia, August 10--13, 2015},
	doi = {10.1145/2783258.2783311},
	editor = {Cao, Longbing and Zhang, Chengqi and Joachims, Thorsten and Webb, Geoffrey I. and Margineantu, Dragos D. and Williams, Graham},
	pages = {259--268},
	publisher = {{ACM}},
	timestamp = {Mon, 26 Jun 2023 20:40:36 +0200},
	title = {Certifying and Removing Disparate Impact},
	url = {https://doi.org/10.1145/2783258.2783311},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1145/2783258.2783311}}

@inproceedings{ZemelWSPD13,
	author = {Zemel, Richard S. and Wu, Yu and Swersky, Kevin and Pitassi, Toniann and Dwork, Cynthia},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/icml/ZemelWSPD13.bib},
	booktitle = {Proceedings of the 30th International Conference on Machine Learning, {ICML} 2013, Atlanta, GA, USA, 16--21 June 2013},
	pages = {325--333},
	publisher = {JMLR.org},
	series = {{JMLR} Workshop and Conference Proceedings},
	timestamp = {Wed, 29 May 2019 08:41:45 +0200},
	title = {Learning Fair Representations},
	url = {http://proceedings.mlr.press/v28/zemel13.html},
	volume = {28},
	year = {2013},
	bdsk-url-1 = {http://proceedings.mlr.press/v28/zemel13.html}}
