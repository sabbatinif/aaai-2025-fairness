@inproceedings{NIPS2017_a486cd07,
 author = {Kusner, Matt J. and Loftus, Joshua and Russell, Chris and Silva, Ricardo},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Counterfactual Fairness},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf},
 volume = {30},
 year = {2017}
}

@book{pearl2009causality,
  title = {Causality: Models, Reasoning and Inference.},
  author = {Pearl, Judea},
  publisher = {Cambridge University Press},
  year = {2000}
}

@inbook{doi:https://doi.org/10.1002/9781118619179.ch3,
author = {Bollen, Kenneth A.},
publisher = {John Wiley \& Sons, Ltd},
isbn = {9781118619179},
title = {Causality and Causal Models},
booktitle = {Structural Equations with Latent Variables},
chapter = {Three},
pages = {40-79},
doi = {https://doi.org/10.1002/9781118619179.ch3},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781118619179.ch3},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118619179.ch3},
year = {1989},
keywords = {causal models, pseudo-isolation},
abstract = {Summary This chapter explores the nature of causality, the conditions of causation, and the limits of causal modeling. It examines threats to isolation or pseudo-isolation. The chapter describes some of the limits of structural equation modeling. The three subsections are (1) model-data versus model-reality consistency, (2) experimental and nonexperimental research designs, and (3) criticisms of structural equation modeling. Two common misconceptions about structural equation models are (1) that these are statistical techniques only for nonexperimental data and (2) that true experiments solve the omitted variable problem that plagues structural equation analyses of nonexperimental data. Finally, the chapter realizes that the problems of demonstrating isolation, association, and direction of causation are age-old issues. The chapter describes them as they are manifested in structural equation models.}
}


@article{10.1145/3527848,
author = {Karimi, Amir-Hossein and Barthe, Gilles and Sch\"{o}lkopf, Bernhard and Valera, Isabel},
title = {A Survey of Algorithmic Recourse: Contrastive Explanations and Consequential Recommendations},
year = {2022},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3527848},
doi = {10.1145/3527848},
abstract = {Machine learning is increasingly used to inform decision making in sensitive situations where decisions have consequential effects on individuals’ lives. In these settings, in addition to requiring models to be accurate and robust, socially relevant values such as fairness, privacy, accountability, and explainability play an important role in the adoption and impact of said technologies. In this work, we focus on algorithmic recourse, which is concerned with providing explanations and recommendations to individuals who are unfavorably treated by automated decision-making systems. We first perform an extensive literature review, and align the efforts of many authors by presenting unified definitions, formulations, and solutions to recourse. Then, we provide an overview of the prospective research directions toward which the community may engage, challenging existing assumptions and making explicit connections to other ethical challenges such as security, privacy, and fairness.},
journal = {ACM Comput. Surv.},
month = {dec},
articleno = {95},
numpages = {29},
keywords = {Algorithmic recourse, contrastive explanations and consequential recommendations}
}

@article{guidotti2022counterfactual,
  title={Counterfactual explanations and how to find them: literature review and benchmarking},
  author={Guidotti, Riccardo},
  journal={Data Mining and Knowledge Discovery},
  volume={},
  number={},
  pages={},
  year={2022},
  publisher={Springer},
  doi={10.1007/s10618-022-00831-6},
  url={https://doi.org/10.1007/s10618-022-00831-6},
  issn={1573-756X},
  abstract={Interpretable machine learning aims at unveiling the reasons behind predictions returned by uninterpretable classifiers. One of the most valuable types of explanation consists of counterfactuals. A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome. For instance, a bank customer asks for a loan that is rejected. The counterfactual explanation consists of what should have been different for the customer in order to have the loan accepted. Recently, there has been an explosion of proposals for counterfactual explainers. The aim of this work is to survey the most recent explainers returning counterfactual explanations. We categorize explainers based on the approach adopted to return the counterfactuals, and we label them according to characteristics of the method and properties of the counterfactuals returned. In addition, we visually compare the explanations, and we report quantitative benchmarking assessing minimality, actionability, stability, diversity, discriminative power, and running time. The results make evident that the current state of the art does not provide a counterfactual explainer able to guarantee all these properties simultaneously.},
  date={2022-04-28},
}


@article{sokol2022fat,
  title={{FAT Forensics}: {A} {Python} Toolbox for Algorithmic Fairness,
         Accountability and Transparency},
  author={Sokol, Kacper and Santos-Rodriguez, Raul and Flach, Peter},
  journal={Software Impacts},
  pages={100406},
  year={2022},
  publisher={Elsevier}
}

@article{goethals2023precof,
  title={PreCoF: counterfactual explanations for fairness},
  author={Goethals, Sofie and Martens, David and Calders, Toon},
  journal={Machine Learning},
  volume={},
  number={},
  pages={},
  year={2023},
  publisher={Springer},
  doi={10.1007/s10994-023-06319-8},
  url={https://doi.org/10.1007/s10994-023-06319-8},
  issn={1573-0565},
  abstract={This paper studies how counterfactual explanations can be used to assess the fairness of a model. Using machine learning for high-stakes decisions is a threat to fairness as these models can amplify bias present in the dataset, and there is no consensus on a universal metric to detect this. The appropriate metric and method to tackle the bias in a dataset will be case-dependent, and it requires insight into the nature of the bias first. We aim to provide this insight by integrating explainable AI (XAI) research with the fairness domain. More specifically, apart from being able to use (Predictive) Counterfactual Explanations to detect explicit bias when the model is directly using the sensitive attribute, we show that it can also be used to detect implicit bias when the model does not use the sensitive attribute directly but does use other correlated attributes leading to a substantial disadvantage for a protected group. We call this metric PreCoF, or Predictive Counterfactual Fairness. Our experimental results show that our metric succeeds in detecting occurrences of implicit bias in the model by assessing which attributes are more present in the explanations of the protected group compared to the unprotected group. These results could help policymakers decide on whether this discrimination is justified or not.},
  date={2023-03-28},
}


@misc{Dua_2019 ,
author = "Dua, Dheeru and Graff, Casey",
year = "2017",
title = "{UCI} Machine Learning Repository",
url = "http://archive.ics.uci.edu/ml",
institution = "University of California, Irvine, School of Information and Computer Sciences" }

@Book{Morik10,
  author =       "K. Morik",
  title =        "Medicine: Applications of Machine Learning",
bookTitle="Encyclopedia of Machine Learning",
  publisher =    "Springer US",
  pages="654--661",
  year =         "2010"
}


@book{10.5555/573193,
author = {Trippi, Robert R. and Turban, Efraim},
title = {Neural Networks in Finance and Investing: Using Artificial Intelligence to Improve Real World Performance},
year = {1992},
isbn = {1557384525},
publisher = {McGraw-Hill, Inc.},
address = {USA},
abstract = {From the Publisher:Neural networks are revolutionizing virtually every aspect of financial and investment decision making. Financial firms worldwide are employing neural networks to tackle difficult tasks involving intuitive judgement or requiring the detection of data patterns which elude conventional analytic techniques. Many observers believe neural networks will eventually outperform even the best traders and investors. Neural networks are already being used to trade the securities markets, to forecast the economy and to analyze credit risk. Indeed, apart from the U.S. Department of Defense, the financial services industry has invested more money in neural network research than any other industry or government body. Unlike other types of artificial intelligence, neural networks mimic to some extent the processing characteristics of the human brain. As a result, neural networks can draw conclusions from incomplete data, recognize patterns as they unfold in real time and forecast the future. They can even learn from past mistakes! In Neural Networks in Finance and Investing, Robert Trippi and Efraim Turban have assembled a stellar collection of articles by experts in industry and academia on the applications of neural networks in this important arena. They discuss neural network successes and failures, as well as identify the vast unrealized potential of neural networks in numerous specialized areas of financial decision making. Topics include neural network fundamentals and overview, analysis of financial condition, business failure prediction, debt risk assessment, security market applications, and neural network approaches to financial forecasting. Nowhere else will the finance professional find such an exciting and relevant in-depth examination of neural networks. Individual chapters discuss how to use neural networks to forecast the stock market, to trade commodities, to assess bond and mortgage risk, to predict bankruptcy and to implement investment strategies. Taken toge}
}

@article{angwin2016machine,
  added-at = {2020-09-14T11:52:21.000+0200},
  author = {Angwin, Julia and Larson, Jeff and Mattu, Surya and Kirchner, Lauren},
  biburl = {https://www.bibsonomy.org/bibtex/23d537ed0185eb7820ed6769aca10acb4/wanlo},
  interhash = {e9260c2f8fdd08ef1a34f7a3a243b0ff},
  intrahash = {3d537ed0185eb7820ed6769aca10acb4},
  journaltitle = {Propublica},
  keywords = {background},
  timestamp = {2020-09-14T11:52:21.000+0200},
  title = {{Machine Bias}},
  url = {https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing},
  year = {2016},
}

@article{DBLP:journals/corr/abs-1905-12728,
  author       = {Fernando Mart{\'{\i}}nez{-}Plumed and
                  C{\`{e}}sar Ferri and
                  David Nieves and
                  Jos{\'{e}} Hern{\'{a}}ndez{-}Orallo},
  title        = {Fairness and Missing Values},
  journal      = {CoRR},
  volume       = {abs/1905.12728},
  year         = {2019},
  url          = {http://arxiv.org/abs/1905.12728},
  eprinttype    = {arXiv},
  eprint       = {1905.12728},
  timestamp    = {Thu, 14 Oct 2021 09:16:55 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1905-12728.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@online{ProPublica2,
  author = {Julia Angwin and Jeff Larson and Mattu Surya and Lauren Kirchner},
  title = {How we analysed the COMPAS recidivism algorithm. \textit{ProPublica}.},
  year = 2016,
  url = {https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm},
  urldate = {2016-05-23}
}

@InProceedings{pmlr-v81-buolamwini18a,
  title = 	 {Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification},
  author = 	 {Buolamwini, Joy and Gebru, Timnit},
  booktitle = 	 {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
  pages = 	 {77--91},
  year = 	 {2018},
  editor = 	 {Friedler, Sorelle A. and Wilson, Christo},
  volume = 	 {81},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--24 Feb},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf},
  url = 	 {https://proceedings.mlr.press/v81/buolamwini18a.html},
}

@inproceedings{10.1145/2702123.2702520,
author = {Kay, Matthew and Matuszek, Cynthia and Munson, Sean A.},
title = {Unequal Representation and Gender Stereotypes in Image Search Results for Occupations},
year = {2015},
isbn = {9781450331456},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2702123.2702520},
doi = {10.1145/2702123.2702520},
booktitle = {Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems},
pages = {3819–3828},
numpages = {10},
location = {Seoul, Republic of Korea},
series = {CHI '15}
}

@article{Leicht-Deobald,
author = {Leicht-Deobald, Ulrich and Busch, Thorsten and Schank, Christoph and Weibel, Antoinette and Schafheitle, Simon and Wildhaber, Isabelle and Kasper, Gabriel},
year = {2019},
month = {12},
pages = {377–392},
title = {The Challenges of Algorithm-Based HR Decision-Making for Personal Integrity},
volume = {160(2)},
journal = {Journal of Business Ethics},
doi = {10.1007/s10551-019-04204-w}
}

@article{act1964civil,
title={Civil Rights Act},
author={{An Act}},
journal={Title VII, Equal Employment Opportunities},
year={1964}
}

@misc{chouldechova2018frontiers,
      title={The Frontiers of Fairness in Machine Learning}, 
      author={Alexandra Chouldechova and Aaron Roth},
      year={2018},
      eprint={1810.08810},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{wang2022brief,
  title={A brief review on algorithmic fairness},
  author={Wang, Xiaomeng and Zhang, Yishi and Zhu, Ruilin},
  journal={Management System Engineering},
  volume={1},
  number={1},
  pages={7},
  year={2022},
  publisher={Springer},
  doi={10.1007/s44176-022-00006-z},
  url={https://doi.org/10.1007/s44176-022-00006-z},
  issn={2731-5843},
  abstract={Machine learning algorithms are widely used in management systems in different fields, such as employee recruitment, loan provision, disease diagnosis, etc., and even in some risky decision-making areas, playing an increasingly crucial role in decisions affecting people’s lives and social development. However, the use of algorithms for automated decision-making can cause unintentional biases that lead to discrimination against certain specific groups. In this context, it is crucial to develop machine learning algorithms that are not only accurate but also fair. There is an extensive discussion of algorithmic fairness in the existing literature. Many scholars have proposed and tested definitions of fairness and attempted to address the problem of unfairness or discrimination in algorithms. This review aims to outline different definitions of algorithmic fairness and to introduce the procedure for constructing fair algorithms to enhance fairness in machine learning. First, this review divides the definitions of algorithmic fairness into two categories, namely, awareness-based fairness and rationality-based fairness, and discusses existing representative algorithmic fairness concepts and notions based on the two categories. Then, metrics for unfairness/discrimination identification are summarized and different unfairness/discrimination removal approaches are discussed to facilitate a better understanding of how algorithmic fairness can be implemented in different scenarios. Challenges and future research directions in the field of algorithmic fairness are finally concluded.},
  date={2022-11-10},
}

@inproceedings{Barocas2018FairnessAM,
  title={Fairness and Machine Learning Limitations and Opportunities},
  author={Solon Barocas and Moritz Hardt and Arvind Narayanan},
  publisher = {MIT Press},
  year={2018},
  url = {https://fairmlbook.org/}
}

@article{Liu_trade,
author = {Liu, Suyun and Vicente, Luis},
year = {2022},
month = {07},
pages = {},
title = {Accuracy and fairness trade-offs in machine learning: a stochastic multi-objective approach},
volume = {19},
journal = {Computational Management Science},
doi = {10.1007/s10287-022-00425-z}
}

@InProceedings{pmlr-v80-agarwal18a,
  title = 	 {A Reductions Approach to Fair Classification},
  author =       {Agarwal, Alekh and Beygelzimer, Alina and Dudik, Miroslav and Langford, John and Wallach, Hanna},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {60--69},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/agarwal18a/agarwal18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/agarwal18a.html},
}

@article{bechavod2018penalizing,
author = {Bechavod, Yahav and Ligett, Katrina},
year = {2017},
month = {06},
pages = {1733–1782},
title = {Learning Fair Classifiers: A Regularization-Inspired Approach}
}

@InProceedings{10.1007/978-3-642-33486-3_3,
author="Kamishima, Toshihiro
and Akaho, Shotaro
and Asoh, Hideki
and Sakuma, Jun",
editor="Flach, Peter A.
and De Bie, Tijl
and Cristianini, Nello",
title="Fairness-Aware Classifier with Prejudice Remover Regularizer",
booktitle="Machine Learning and Knowledge Discovery in Databases",
year="2012",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="35--50",
}

@inproceedings{10.1145/3038912.3052660,
author = {Zafar, Muhammad Bilal and Valera, Isabel and Gomez Rodriguez, Manuel and Gummadi, Krishna P.},
title = {Fairness Beyond Disparate Treatment \& Disparate Impact: Learning Classification without Disparate Mistreatment},
year = {2017},
isbn = {9781450349130},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3038912.3052660},
doi = {10.1145/3038912.3052660},
booktitle = {Proceedings of the 26th International Conference on World Wide Web},
pages = {1171–1180},
numpages = {10},
location = {Perth, Australia},
series = {WWW '17}
}

@InProceedings{pmlr-v54-zafar17a,
  title = 	 {{Fairness Constraints: Mechanisms for Fair Classification}},
  author = 	 {Zafar, Muhammad Bilal and Valera, Isabel and Rogriguez, Manuel Gomez and Gummadi, Krishna P.},
  booktitle = 	 {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {962--970},
  year = 	 {2017},
  editor = 	 {Singh, Aarti and Zhu, Jerry},
  volume = 	 {54},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {20--22 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v54/zafar17a/zafar17a.pdf},
  url = 	 {https://proceedings.mlr.press/v54/zafar17a.html},
}

@inproceedings{NIPS2016_9d268236,
 author = {Hardt, Moritz and Price, Eric and Price, Eric and Srebro, Nati},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Equality of Opportunity in Supervised Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2016/file/9d2682367c3935defcb1f9e247a97c0d-Paper.pdf},
 volume = {29},
 year = {2016}
}

@article{10.2307/24758720,
 ISSN = {00081221},
 URL = {http://www.jstor.org/stable/24758720},
 author = {Solon Barocas and Andrew D. Selbst},
 journal = {California Law Review},
 number = {3},
 pages = {671--732},
 publisher = {California Law Review, Inc.},
 title = {Big Data's Disparate Impact},
 urldate = {2023-04-24},
 volume = {104},
 year = {2016}
}

@InProceedings{pmlr-v81-dwork18a,
  title = 	 {Decoupled Classifiers for Group-Fair and Efficient Machine Learning},
  author = 	 {Dwork, Cynthia and Immorlica, Nicole and Kalai, Adam Tauman and Leiserson, Max},
  booktitle = 	 {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
  pages = 	 {119--133},
  year = 	 {2018},
  editor = 	 {Friedler, Sorelle A. and Wilson, Christo},
  volume = 	 {81},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--24 Feb},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v81/dwork18a/dwork18a.pdf},
  url = 	 {https://proceedings.mlr.press/v81/dwork18a.html},
}

@misc{EU-GDPR-2016,
    title = {{EU General Data Protection Regulation (GDPR): Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC (General Data Protection Regulation)}},
    author = {{European Union}},
    year = {2016},
    howpublished = {\url{http://data.europa.eu/eli/reg/2016/679/2016-05-04}},
    note = {Official Journal of the European Union}
}

@article{wachter2017counterfactual,
  title={Counterfactual Explanations without Opening the Black Box: Automated Decisions and the GDPR},
  author={Wachter, Sandra and Mittelstadt, Brent and Russell, Chris},
  journal={Harvard Journal of Law \& Technology (Harvard JOLT)},
  volume={31},
  number={2},
  pages={841--888},
  year={2017},
  publisher={Harvard Journal of Law \& Technology},
  language={English},
  url={https://heinonline.org/HOL/P?h=hein.journals/hjlt31&i=859},
  urldate={2024-02-17},
}


@inproceedings{10.1145/3351095.3372864,
author = {Binns, Reuben},
title = {On the apparent conflict between individual and group fairness},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372864},
doi = {10.1145/3351095.3372864},
abstract = {A distinction has been drawn in fair machine learning research between 'group' and 'individual' fairness measures. Many technical research papers assume that both are important, but conflicting, and propose ways to minimise the trade-offs between these measures. This paper argues that this apparent conflict is based on a misconception. It draws on discussions from within the fair machine learning research, and from political and legal philosophy, to argue that individual and group fairness are not fundamentally in conflict. First, it outlines accounts of egalitarian fairness which encompass plausible motivations for both group and individual fairness, thereby suggesting that there need be no conflict in principle. Second, it considers the concept of individual justice, from legal philosophy and jurisprudence, which seems similar but actually contradicts the notion of individual fairness as proposed in the fair machine learning literature. The conclusion is that the apparent conflict between individual and group fairness is more of an artefact of the blunt application of fairness measures, rather than a matter of conflicting principles. In practice, this conflict may be resolved by a nuanced consideration of the sources of 'unfairness' in a particular deployment context, and the carefully justified application of measures to mitigate it.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {514–524},
numpages = {11},
keywords = {statistical parity, machine learning, justice, individual fairness, fairness, discrimination},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/2090236.2090255,
author = {Dwork, Cynthia and Hardt, Moritz and Pitassi, Toniann and Reingold, Omer and Zemel, Richard},
title = {Fairness through awareness},
year = {2012},
isbn = {9781450311151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2090236.2090255},
doi = {10.1145/2090236.2090255},
abstract = {We study fairness in classification, where individuals are classified, e.g., admitted to a university, and the goal is to prevent discrimination against individuals based on their membership in some group, while maintaining utility for the classifier (the university). The main conceptual contribution of this paper is a framework for fair classification comprising (1) a (hypothetical) task-specific metric for determining the degree to which individuals are similar with respect to the classification task at hand; (2) an algorithm for maximizing utility subject to the fairness constraint, that similar individuals are treated similarly. We also present an adaptation of our approach to achieve the complementary goal of "fair affirmative action," which guarantees statistical parity (i.e., the demographics of the set of individuals receiving any classification are the same as the demographics of the underlying population), while treating similar individuals as similarly as possible. Finally, we discuss the relationship of fairness to privacy: when fairness implies privacy, and how tools developed in the context of differential privacy may be applied to fairness.},
booktitle = {Proceedings of the 3rd Innovations in Theoretical Computer Science Conference},
pages = {214–226},
numpages = {13},
location = {Cambridge, Massachusetts},
series = {ITCS '12}
}

@article{10.1145/3236009,
author = {Guidotti, Riccardo and Monreale, Anna and Ruggieri, Salvatore and Turini, Franco and Giannotti, Fosca and Pedreschi, Dino},
title = {A Survey of Methods for Explaining Black Box Models},
year = {2018},
issue_date = {September 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3236009},
doi = {10.1145/3236009},
abstract = {In recent years, many accurate decision support systems have been constructed as black boxes, that is as systems that hide their internal logic to the user. This lack of explanation constitutes both a practical and an ethical issue. The literature reports many approaches aimed at overcoming this crucial weakness, sometimes at the cost of sacrificing accuracy for interpretability. The applications in which black box decision systems can be used are various, and each approach is typically developed to provide a solution for a specific problem and, as a consequence, it explicitly or implicitly delineates its own definition of interpretability and explanation. The aim of this article is to provide a classification of the main problems addressed in the literature with respect to the notion of explanation and the type of black box system. Given a problem definition, a black box type, and a desired explanation, this survey should help the researcher to find the proposals more useful for his own work. The proposed classification of approaches to open black box models should also be useful for putting the many research open questions in perspective.},
journal = {ACM Comput. Surv.},
month = {aug},
articleno = {93},
numpages = {42},
keywords = {Open the black box, explanations, interpretability, transparent models}
}

@book{byrne2007rational,
  title={The Rational Imagination: How People Create Alternatives to Reality},
  author={Byrne, Ruth M.J.},
  year={2007},
  publisher={MIT Press}
}


@article{verma2020counterfactual,
  title={Counterfactual explanations for machine learning: A review},
  author={Verma, Sahil and Dickerson, John and Hines, Kathryn},
  year={2020},
  eprint={2010.10596},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}

@article{10.1145/3494672,
author = {Pessach, Dana and Shmueli, Erez},
title = {A Review on Fairness in Machine Learning},
year = {2022},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3494672},
doi = {10.1145/3494672},
abstract = {An increasing number of decisions regarding the daily lives of human beings are being controlled by artificial intelligence and machine learning (ML) algorithms in spheres ranging from healthcare, transportation, and education to college admissions, recruitment, provision of loans, and many more realms. Since they now touch on many aspects of our lives, it is crucial to develop ML algorithms that are not only accurate but also objective and fair. Recent studies have shown that algorithmic decision making may be inherently prone to unfairness, even when there is no intention for it. This article presents an overview of the main concepts of identifying, measuring, and improving algorithmic fairness when using ML algorithms, focusing primarily on classification tasks. The article begins by discussing the causes of algorithmic bias and unfairness and the common definitions and measures for fairness. Fairness-enhancing mechanisms are then reviewed and divided into pre-process, in-process, and post-process mechanisms. A comprehensive comparison of the mechanisms is then conducted, toward a better understanding of which mechanisms should be used in different scenarios. The article ends by reviewing several emerging research sub-fields of algorithmic fairness, beyond classification.},
journal = {ACM Comput. Surv.},
month = {feb},
articleno = {51},
numpages = {44},
keywords = {fairness in machine learning, fairness-aware machine learning, algorithmic fairness, Algorithmic bias}
}

@ARTICLE{9321372,
  author={Stepin, Ilia and Alonso, Jose M. and Catala, Alejandro and Pereira-Fariña, Martín},
  journal={IEEE Access}, 
  title={A Survey of Contrastive and Counterfactual Explanation Generation Methods for Explainable Artificial Intelligence}, 
  year={2021},
  volume={9},
  number={},
  pages={11974-12001},
  abstract={A number of algorithms in the field of artificial intelligence offer poorly interpretable decisions. To disclose the reasoning behind such algorithms, their output can be explained by means of so-called evidence-based (or factual) explanations. Alternatively, contrastive and counterfactual explanations justify why the output of the algorithms is not any different and how it could be changed, respectively. It is of crucial importance to bridge the gap between theoretical approaches to contrastive and counterfactual explanation and the corresponding computational frameworks. In this work we conduct a systematic literature review which provides readers with a thorough and reproducible analysis of the interdisciplinary research field under study. We first examine theoretical foundations of contrastive and counterfactual accounts of explanation. Then, we report the state-of-the-art computational frameworks for contrastive and counterfactual explanation generation. In addition, we analyze how grounded such frameworks are on the insights from the inspected theoretical approaches. As a result, we highlight a variety of properties of the approaches under study and reveal a number of shortcomings thereof. Moreover, we define a taxonomy regarding both theoretical and practical approaches to contrastive and counterfactual explanation.},
  keywords={Cognition;Artificial intelligence;Training;Terminology;Taxonomy;Systematics;Signal to noise ratio;Computational intelligence;contrastive explanations;counterfactuals;explainable artificial intelligence;systematic literature review},
  doi={10.1109/ACCESS.2021.3051315},
  ISSN={2169-3536},
  month={},}

@article{artelt2019computation,
  title={On the computation of counterfactual explanations--A survey},
  author={Artelt, Andr{\'e} and Hammer, Barbara},
  journal={arXiv preprint arXiv:1911.07749},
  year={2019}
}

@misc{karimi2021survey,
      title={A survey of algorithmic recourse: definitions, formulations, solutions, and prospects}, 
      author={Amir-Hossein Karimi and Gilles Barthe and Bernhard Schölkopf and Isabel Valera},
      year={2021},
      eprint={2010.04050},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@inproceedings{ijcai2019p199,
  title     = {Counterfactual Fairness: Unidentification, Bound and Algorithm},
  author    = {Wu, Yongkai and Zhang, Lu and Wu, Xintao},
  booktitle = {Proceedings of the Twenty-Eighth International Joint Conference on
               Artificial Intelligence, {IJCAI-19}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  pages     = {1438--1444},
  year      = {2019},
  month     = {7},
  doi       = {10.24963/ijcai.2019/199},
  url       = {https://doi.org/10.24963/ijcai.2019/199},
}

@article{Zhang_Bareinboim_2018, title={Fairness in Decision-Making — The Causal Explanation Formula}, volume={32}, url={https://ojs.aaai.org/index.php/AAAI/article/view/11564}, DOI={10.1609/aaai.v32i1.11564}, abstractNote={ &lt;p&gt; AI plays an increasingly prominent role in society since decisions that were once made by humans are now delegated to automated systems. These systems are currently in charge of deciding bank loans, criminals’ incarceration, and the hiring of new employees, and it’s not difficult to envision that they will in the future underpin most of the decisions in society. Despite the high complexity entailed by this task, there is still not much understanding of basic properties of such systems. For instance, we currently cannot detect (neither explain nor correct) whether an AI system can be deemed fair (i.e., is abiding by the decision-constraints agreed by society) or it is reinforcing biases and perpetuating a preceding prejudicial practice. Issues of discrimination have been discussed extensively in political and legal circles, but there exists still not much understanding of the formal conditions that a system must meet to be deemed fair. In this paper, we use the language of structural causality (Pearl, 2000) to fill in this gap. We start by introducing three new fine-grained measures of transmission of change from stimulus to effect, which we called counterfactual direct (Ctf-DE), indirect (Ctf-IE), and spurious (Ctf-SE) effects. We then derive what we call the causal explanation formula, which allows the AI designer to quantitatively evaluate fairness and explain the total observed disparity of decisions through different discriminatory mechanisms. We apply these measures to various discrimination analysis tasks and run extensive simulations, including detection, evaluation, and optimization of decision-making under fairness constraints. We conclude studying the trade-off between different types of fairness criteria (outcome and procedural), and provide a quantitative approach to policy implementation and the design of fair AI systems. &lt;/p&gt; }, number={1}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Zhang, Junzhe and Bareinboim, Elias}, year={2018}, month={Apr.} }

@article{Chiappa_2019, title={Path-Specific Counterfactual Fairness}, volume={33}, url={https://ojs.aaai.org/index.php/AAAI/article/view/4777}, DOI={10.1609/aaai.v33i01.33017801}, abstractNote={&lt;p&gt;We consider the problem of learning fair decision systems from data in which a sensitive attribute might affect the decision along both fair and unfair pathways. We introduce a counterfactual approach to disregard effects along unfair pathways that does not incur in the same loss of individual-specific information as previous approaches. Our method corrects observations adversely affected by the sensitive attribute, and uses these to form a decision. We leverage recent developments in deep learning and approximate inference to develop a VAE-type method that is widely applicable to complex nonlinear models.&lt;/p&gt;}, number={01}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Chiappa, Silvia}, year={2019}, month={Jul.}, pages={7801-7808} }

@inproceedings{pan2021explaining,
  title={Explaining algorithmic fairness through fairness-aware causal path decomposition},
  author={Pan, Wei and Cui, Shiyang and Bian, Jiang and Zhang, Cheng and Wang, Fei},
  booktitle={Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \& Data Mining},
  pages={1287--1297},
  year={2021}
}


@inproceedings{10.1609/aaai.v37i9.26344,
author = {Zhao, Yuying and Wang, Yu and Derr, Tyler},
title = {Fairness and explainability: bridging the gap towards fair model explanations},
year = {2023},
isbn = {978-1-57735-880-0},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v37i9.26344},
doi = {10.1609/aaai.v37i9.26344},
abstract = {While machine learning models have achieved unprecedented success in real-world applications, they might make biased/unfair decisions for specific demographic groups and hence result in discriminative outcomes. Although research efforts have been devoted to measuring and mitigating bias, they mainly study bias from the result-oriented perspective while neglecting the bias encoded in the decision-making procedure. This results in their inability to capture procedure-oriented bias, which therefore limits the ability to have a fully debiasing method. Fortunately, with the rapid development of explainable machine learning, explanations for predictions are now available to gain insights into the procedure. In this work, we bridge the gap between fairness and explainability by presenting a novel perspective of procedure-oriented fairness based on explanations. We identify the procedure-based bias by measuring the gap of explanation quality between different groups with Ratio-based and Value-based Explanation Fairness. The new metrics further motivate us to design an optimization objective to mitigate the procedure-based bias where we observe that it will also mitigate bias from the prediction. Based on our designed optimization objective, we propose a Comprehensive Fairness Algorithm (CFA), which simultaneously fulfills multiple objectives - improving traditional fairness, satisfying explanation fairness, and maintaining the utility performance. Extensive experiments on real-world datasets demonstrate the effectiveness of our proposed CFA and highlight the importance of considering fairness from the explainability perspective.},
booktitle = {Proceedings of the Thirty-Seventh AAAI Conference on Artificial Intelligence and Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence and Thirteenth Symposium on Educational Advances in Artificial Intelligence},
articleno = {1275},
numpages = {9},
series = {AAAI'23/IAAI'23/EAAI'23}
}

@online{loandataset,
  author      = {Burak Ergün},
  title       = {Predict Loan Eligibility for Dream Housing Finance company},
  url         = {https://datahack.analyticsvidhya.com/contest/practice-problem-loan-prediction-iii/},
  urldate     = {2016-05-26},
}

@misc{europeancommission2021,
  author       = {{European Commission}},
  title        = {{Proposal for a Regulation of the European Parliament and of the Council laying down harmonised rules on Artificial Intelligence (Artificial Intelligence Act) and amending certain Union legislative acts}},
  year         = {2021},
  howpublished = {https://digital-strategy.ec.europa.eu/en/library/proposal-regulationlaying-down-harmonised-rules-artificial-intelligence},
}

@article{mucsanyi2023trustworthy,
  title={Trustworthy Machine Learning},
  author={Mucs{\'a}nyi, B{\'a}lint and Kirchhof, Michael and Nguyen, Elisa and Rubinstein, Alexander and Oh, Seong Joon},
  journal={arXiv preprint arXiv:2310.08215},
  year={2023}
}

@ARTICLE{8466590,
  author={Adadi, Amina and Berrada, Mohammed},
  journal={IEEE Access}, 
  title={Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence (XAI)}, 
  year={2018},
  volume={6},
  number={},
  pages={52138-52160},
  abstract={At the dawn of the fourth industrial revolution, we are witnessing a fast and widespread adoption of artificial intelligence (AI) in our daily life, which contributes to accelerating the shift towards a more algorithmic society. However, even with such unprecedented advancements, a key impediment to the use of AI-based systems is that they often lack transparency. Indeed, the black-box nature of these systems allows powerful predictions, but it cannot be directly explained. This issue has triggered a new debate on explainable AI (XAI). A research field holds substantial promise for improving trust and transparency of AI-based systems. It is recognized as the sine qua non for AI to continue making steady progress without disruption. This survey provides an entry point for interested researchers and practitioners to learn key aspects of the young and rapidly growing body of research related to XAI. Through the lens of the literature, we review the existing approaches regarding the topic, discuss trends surrounding its sphere, and present major research trajectories.},
  keywords={Conferences;Machine learning;Market research;Prediction algorithms;Machine learning algorithms;Biological system modeling;Explainable artificial intelligence;interpretable machine learning;black-box models},
  doi={10.1109/ACCESS.2018.2870052},
  ISSN={2169-3536},
  month={},}

@article{10.1145/3616865,
author = {Caton, Simon and Haas, Christian},
title = {Fairness in Machine Learning: A Survey},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {0360-0300},
url = {https://doi.org/10.1145/3616865},
doi = {10.1145/3616865},
abstract = {When Machine Learning technologies are used in contexts that affect citizens, companies as well as researchers need to be confident that there will not be any unexpected social implications, such as bias towards gender, ethnicity, and/or people with disabilities. There is significant literature on approaches to mitigate bias and promote fairness, yet the area is complex and hard to penetrate for newcomers to the domain. This article seeks to provide an overview of the different schools of thought and approaches that aim to increase the fairness of Machine Learning. It organises approaches into the widely accepted framework of pre-processing, in-processing, and post-processing methods, subcategorizing into a further 11 method areas. Although much of the literature emphasizes binary classification, a discussion of fairness in regression, recommender systems, and unsupervised learning is also provided along with a selection of currently available open source libraries. The article concludes by summarising open challenges articulated as five dilemmas for fairness research.},
note = {Just Accepted},
journal = {ACM Comput. Surv.},
month = {aug},
keywords = {transparency, machine learning, fairness, accountability}
}

@misc{corbettdavies2023measure,
      title={The Measure and Mismeasure of Fairness}, 
      author={Sam Corbett-Davies and Johann D. Gaebler and Hamed Nilforoshan and Ravi Shroff and Sharad Goel},
      year={2023},
      eprint={1808.00023},
      archivePrefix={arXiv},
      primaryClass={cs.CY}
}

@article{psyke-ia2022,
	author = {Sabbatini, Federico and Ciatto, Giovanni and Calegari, Roberta and Omicini, Andrea},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/ia/SabbatiniCCO22.bib},
	doi = {10.3233/IA-210120},
	journal = {Intelligenza Artificiale},
	number = {1},
	pages = {27--48},
	timestamp = {Sun, 24 Jul 2022 12:56:47 +0200},
	title = {Symbolic Knowledge Extraction from Opaque {ML} Predictors in {PSyKE}: Platform Design {\&} Experiments},
	url = {https://doi.org/10.3233/IA-210120},
	volume = {16},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.3233/IA-210120}}

@inproceedings{gridrex-kr2022,
	author = {Sabbatini, Federico and Calegari, Roberta},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/kr/SabbatiniC22.bib},
	booktitle = {Proceedings of the 19th International Conference on Principles of Knowledge Representation and Reasoning, {KR} 2022, Haifa, Israel. July 31 - August 5, 2022},
	editor = {Kern{-}Isberner, Gabriele and Lakemeyer, Gerhard and Meyer, Thomas},
	timestamp = {Wed, 27 Jul 2022 16:12:10 +0200},
	title = {Symbolic Knowledge Extraction from Opaque Machine Learning Predictors: {GridREx} {\&} {PEDRO}},
	url = {https://proceedings.kr.org/2022/57/},
	doi = {10.24963/kr.2022/57},
	year = {2022},
	bdsk-url-1 = {https://proceedings.kr.org/2022/57/}}

@incollection{gridex-extraamas2021,
	_url = {http://link.springer.com/10.1007/978-3-030-82017-6_2},
	address = {Basel, Switzerland},
	author = {Sabbatini, Federico and Ciatto, Giovanni and Omicini, Andrea},
	booktitle = {Explainable and Transparent AI and Multi-Agent Systems. Third International Workshop, EXTRAAMAS 2021, Virtual Event, May 3--7, 2021, Revised Selected Papers},
	date-modified = {2021-09-17 17:29:10 +0200},
	doi = {10.1007/978-3-030-82017-6_2},
	editor = {Calvaresi, Davide and Najjar, Amro and Winikoff, Michael and Fr{\"a}mling, Kary},
	isbn = {978-3-030-82016-9},
	isbn-online = {978-3-030-82017-6},
	issn = {0302-9743},
	issn-online = {1611-3349},
	pages = {18--38},
	publisher = {Springer Nature},
	series = {LNCS},
	subseries = {Lecture Notes in Artificial Intelligence},
	title = {{GridEx}: An Algorithm for Knowledge Extraction from Black-Box Regressors},
	volume = 12688,
	year = 2021,
	bdsk-url-1 = {https://doi.org/10.1007/978-3-030-82017-6_2}}

@article{psyke-trust-aixia2022,
	address = {Cham, Switzerland},
	author = {Calegari, Roberta and Sabbatini, Federico},
	booktitle = {AIxIA 2022},
	doi = {10.1007/978-3-031-27181-6_1},
	editor = {Dovier, Agostino and Montanari, Angelo and Orlandini, Andrea},
	eisbn = {978-3-031-27181-6},
	institution = {University of Udine},
	isbn = {978-3-031-27180-9},
	issn = {0302-9743},
	location = {Udine, Italy},
	month = mar,
	note = {XXI International Conference of the Italian Association for Artificial Intelligence, AIxIA 2022, Udine, Italy, November 28 -- December 2, 2022, Proceedings},
	numpages = 14,
	pages = {3--16},
	publisher = {Springer},
	series = {Lecture Notes in Computer Science},
	subseries = {Lecture Notes in Artificial Intelligence},
	title = {The {PSyKE} Technology for Trustworthy Artificial Intelligence},
	url = {https://doi.org/10.1007/978-3-031-27181-6_1},
	volume = 13796,
	year = 2023,
	bdsk-url-1 = {https://doi.org/10.1007/978-3-031-27181-6_1}}

@article{SKESKISLR2024,
	author       = {Ciatto, Giovanni and Sabbatini, Federico and Agiollo, Andrea and Magnini, Matteo and Omicini, Andrea},
	title        = {Symbolic Knowledge Extraction and Injection with Sub-symbolic Predictors:
	{A} Systematic Literature Review},
	journal      = {{ACM} Computing Surveys},
	volume       = {56},
	number       = {6},
	pages        = {161:1--161:35},
	year         = {2024},
	url          = {https://doi.org/10.1145/3645103},
	doi          = {10.1145/3645103},
	timestamp    = {Mon, 15 Apr 2024 08:25:48 +0200},
	biburl       = {https://dblp.org/rec/journals/csur/CiattoSAMO24.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@book{Wiggins2020calculating,
	author = {Wiggins, Benjamin},
	title = {Calculating Race: Racial Discrimination in Risk Assessment},
	publisher = {Oxford University Press},
	year = {2020},
	month = {11},
	isbn = {9780197504000},
	doi = {10.1093/oso/9780197504000.001.0001},
	url = {https://doi.org/10.1093/oso/9780197504000.001.0001},
}

@article{NuenenASC20,
	author       = {van Nuenen, Tom and Ferrer, Xavier and Such, Jose M. and Cot{\'{e}}, Mark},
	title        = {Transparency for Whom? Assessing Discriminatory Artificial Intelligence},
	journal      = {Computer},
	volume       = {53},
	number       = {11},
	pages        = {36--44},
	year         = {2020},
	url          = {https://doi.org/10.1109/MC.2020.3002181},
	doi          = {10.1109/MC.2020.3002181},
	timestamp    = {Tue, 29 Dec 2020 18:15:24 +0100},
	biburl       = {https://dblp.org/rec/journals/computer/NuenenASC20.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{QuyRIZN22,
	author       = {Quy, Tai Le and Roy, Arjun and Iosifidis, Vasileios and Zhang, Wenbin and Ntoutsi, Eirini},
	title        = {A survey on datasets for fairness-aware machine learning},
	journal      = {WIREs Data Mining and Knowledge Discovery},
	volume       = {12},
	number       = {3},
	year         = {2022},
	url          = {https://doi.org/10.1002/widm.1452},
	doi          = {10.1002/WIDM.1452},
	timestamp    = {Tue, 12 Sep 2023 07:58:39 +0200},
	biburl       = {https://dblp.org/rec/journals/widm/QuyRIZN22.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{ChakrabortyPM20,
	author       = {Chakraborty, Joymallya and Peng, Kewen and Menzies, Tim},
	title        = {Making Fair {ML} Software using Trustworthy Explanation},
	booktitle    = {35th {IEEE/ACM} International Conference on Automated Software Engineering,
	{ASE} 2020, Melbourne, Australia, September 21--25, 2020},
	pages        = {1229--1233},
	publisher    = {{IEEE}},
	year         = {2020},
	url          = {https://doi.org/10.1145/3324884.3418932},
	doi          = {10.1145/3324884.3418932},
	timestamp    = {Mon, 05 Feb 2024 20:31:35 +0100},
	biburl       = {https://dblp.org/rec/conf/kbse/ChakrabortyPM20.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{FeldmanFMSV15,
	author       = {Feldman, Michael and Friedler, Sorelle A. and Moeller, John and Scheidegger, Carlos and Venkatasubramanian, Suresh},
	editor       = {Cao, Longbing and Zhang, Chengqi and Joachims, Thorsten and Webb, Geoffrey I. and Margineantu, Dragos D. and Williams, Graham},
	title        = {Certifying and Removing Disparate Impact},
	booktitle    = {Proceedings of the 21th {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data Mining, Sydney, NSW, Australia, August 10--13, 2015},
	pages        = {259--268},
	publisher    = {{ACM}},
	year         = {2015},
	url          = {https://doi.org/10.1145/2783258.2783311},
	doi          = {10.1145/2783258.2783311},
	timestamp    = {Mon, 26 Jun 2023 20:40:36 +0200},
	biburl       = {https://dblp.org/rec/conf/kdd/FeldmanFMSV15.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{ZemelWSPD13,
	author       = {Zemel, Richard S. and Wu, Yu and Swersky, Kevin and Pitassi, Toniann and Dwork, Cynthia},
	title        = {Learning Fair Representations},
	booktitle    = {Proceedings of the 30th International Conference on Machine Learning, {ICML} 2013, Atlanta, GA, USA, 16--21 June 2013},
	series       = {{JMLR} Workshop and Conference Proceedings},
	volume       = {28},
	pages        = {325--333},
	publisher    = {JMLR.org},
	year         = {2013},
	url          = {http://proceedings.mlr.press/v28/zemel13.html},
	timestamp    = {Wed, 29 May 2019 08:41:45 +0200},
	biburl       = {https://dblp.org/rec/conf/icml/ZemelWSPD13.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
