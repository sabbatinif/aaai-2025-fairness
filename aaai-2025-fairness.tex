%File: anonymous-submission-latex-2025.tex
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage[submission]{aaai25}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in} % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in} % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}

%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\usepackage{latexsym}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage[bb=boondox]{mathalfa}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{color}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{cleveref}

\newtheorem{definition}{Definition}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=none,numberstyle=\footnotesize,xleftmargin=0em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2025.1)
}

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai25.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\title{Individual and Group Fairness Assessments via Counterfactual Explanations}
\author{
    Federico Sabbatini\textsuperscript{\rm 1}, Roberta Calegari\textsuperscript{\rm 2}
}
\affiliations{
    %Afiliations
    \textsuperscript{\rm 1}University of Urbino\\
    \textsuperscript{\rm 2}University of Bologna\\
}

\begin{document}

\maketitle

\begin{abstract}
This study explores the potential of counterfactual explanations to assess AI fairness, especially in critical decision-making systems. Predictive models may amplify biases inherent in datasets or algorithms, and given the absence of a universally accepted fairness metric, a case-specific approach becomes mandatory. 
%
Existing statistical fairness metrics often lack legal compliance or define fairness around thresholds that defy universal definitions and are not legally sound.
%
The goal of this work is to define a measure of fairness for AI systems based on explainable artificial intelligence concepts. Specifically, it leverages the analysis of counterfactual explanations of individuals/groups and their comparison with similar individuals/groups. Compared to existing state-of-the-art works, the contributions are \textit{i)} extending the definition of individual fairness, not limiting unfairness to decisions based on sensitive attributes but also ensuring similar treatment amongst similar individuals; \textit{ii)} revisiting (and generalising) existing notions and introducing new, more refined notions of group fairness based on counterfactuals; \textit{iii)} defining quantitative fairness metrics that reflect the evidence gathered through the analysis/comparison of counterfactual explanations.
\end{abstract}

% Uncomment the following to link to your code, datasets, an extended version or similar.
%
% \begin{links}
%     \link{Code}{https://aaai.org/example/code}
%     \link{Datasets}{https://aaai.org/example/datasets}
%     \link{Extended version}{https://aaai.org/example/extended-version}
% \end{links}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}\label{sec:introduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Over the past decade, automated decision-making systems have seen increasing use in high-steak real-world domains, with significant impacts on people's lives, e.g., healthcare \cite{Morik10}, finance \cite{10.5555/573193}, and criminal justice \cite{angwin2016machine,DBLP:journals/corr/abs-1905-12728}. While artificial intelligence (AI) and machine learning (ML) have demonstrated effectiveness in these predictive processes, concerns are growing regarding their trustworthiness, and in particular about transparency and discriminatory biases in predictions.
%
In response, the European Commission has proposed the first comprehensive legislation on AI systems, known as the AI Act, aimed at establishing a common regulatory and legal framework to ensure the trustworthiness of AI systems before their availability on the European Union market \cite{europeancommission2021}. Amongst the fundamental principles underlying the AI Act are fairness, non-discrimination, and transparency (understood as explainability and traceability).

Many research efforts have focused on these topics, both separately and in a combined way, to leverage methods synergically ensuring the trustworthiness requirements of AI systems \cite{mucsanyi2023trustworthy}.
%
In particular, explainable artificial intelligence (XAI) techniques \cite{8466590} can be employed to understand AI systems and highlight the presence of unfair or discriminatory behaviours, thereby enabling the achievement of \textit{fair ML} \cite{10.1145/3616865}.
%
According to this research direction, we propose studying counterfactual explanations from methods in the XAI field to detect and measure unfairness in AI-based decisions. In a nutshell, counterfactual explanations are hypothetical scenarios identifying the input features to be changed to obtain a different outcome, e.g., from rejection to acceptance \cite{guidotti2022counterfactual,10.1145/3527848}. 

The analysis of counterfactuals highlights whether the requirements to reverse a decision can be considered free from discrimination. Existing works on fairness via counterfactuals are often limited in scope and typically focus only on ensuring the concept of individual fairness \cite{NIPS2017_a486cd07,wachter2017counterfactual}.
%
Depending on the domain, these approaches can be computationally expensive or even unfeasible.
%
%Furthermore, current counterfactual fairness measurements are applied only at the individual level and not to express group fairness \cite{wachter2017counterfactual}.
%
The contributions of this work aim at enhancing the current literature on counterfactual fairness.
%
First, we extend the definition of individual fairness, not limiting unfairness to decisions based on sensitive attributes but also ensuring similar treatment amongst similar individuals\footnote{In this work, we adopt the terms ``protected'' and ``sensitive'' as synonyms, since the fairness considerations discussed here apply to both categories}.
%
Consistent with the literature, an explicit bias is highlighted if counterfactuals involve sensitive attributes, meaning that decisions are based on those attributes. Additionally, we introduce a stronger formulation of fairness, ensuring equal treatment for decision reversals. Unequal treatment is revealed if counterfactuals for similar individuals (i.e., equally qualified) who receive the same outcomes are not identical, indicating that similar individuals are required to accomplish different actions to obtain a decision reversal.
%
Second, we formulate the same fairness definitions for groups, revisiting and generalising existing notions of group fairness based on counterfactuals.
%
Finally, we define quantitative, computationally feasible fairness metrics based on these definitions that reflect the evidence gathered through the analysis/comparison of counterfactual explanations.

\paragraph{Motivating example}
We introduce a running example to better understand the scenario and our definitions. A bank company needs to select individuals to grant loans from a large pool of applicants. An ML predictive algorithm, based on applicants' data (e.g., yearly income, occupation, education, age, and gender), is thus developed to make decisions. Specifically, the system predicts whether a candidate would have the possibility to repay the loan, and those individuals are granted a loan. However, the algorithm tends to generate more positive predictions for one gender (e.g., males) compared to others (e.g., females and non-binary individuals). In particular, John and Mark, male candidates, receive positive outcomes, whereas Clara and Mary, female candidates, receive rejections, even though all four individuals appear qualified to receive the loan. To address this bias, the company implements fair ML techniques to mitigate the issue. Unfortunately, as a result of this modification, John is denied a loan, seemingly without explanation. In this new scenario, Mark and Clara receive the loan, while John and Mary do not. John raises a complaint, citing examples of individuals, Clara and Mark, who were granted loans despite possessing qualifications similar to his. Should the company persist in its position or should it modify the model once again to ensure fairness?

Generally speaking, when considering a situation where a loan application is declined, a fair decision entails that the explanation provided by the company about the required changes to receive credit remains independent of sensitive attributes (e.g., gender, or ethnicity). Accordingly, the counterfactual explanation for John receiving the loan should be produced by the system, and if no sensitive attribute is contained, the decision should be considered fair. This is what existing works usually address.
%
However, we advocate this is not enough, and the company should be able to prove that similar individuals (or groups) are treated similarly, i.e., the counterfactuals of similar individuals are exactly the same and protected features are not taken into account to assess similarity. It means that if Mary is similar to John (i.e., their non-protected attributes are similar) and they both receive a negative prediction, they should have the same requirements to reverse the decision (i.e., identical counterfactuals).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Works}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Discrimination has been identified in AI systems operation in various domains, e.g., justice \cite{ProPublica2}, facial recognition \cite{pmlr-v81-buolamwini18a}, search engines \cite{10.1145/2702123.2702520}, and recruitment \cite{Leicht-Deobald}, highlighting the importance of ethics in the development and use of these tools for governments and societies \cite{act1964civil}.
Recent studies have proven that data-driven algorithms can inherit, reproduce, or even amplify existing patterns in the data related to inequalities between different demographic groups of age, gender, or ethnicity \cite{chouldechova2018frontiers,10.1145/2090236.2090255}. 

Although fairness in ML \cite{Barocas2018FairnessAM} is a relatively recent field of research, several approaches in the literature have already proposed some formalisations of equity within AI systems \cite{wang2022brief,10.1145/3494672}.
%
In a nutshell, existing approaches work on the concepts of \emph{protected attributes}, containing personal information (e.g., gender, religion, race) that should not influence the decision, and of \emph{sensitive attributes}, that may be correlated with protected attributes and thus exploited as proxies \cite{QuyRIZN22,NuenenASC20,Wiggins2020calculating}.
%
After having determined the protected/sensitive features, a notion of (individual or group) fairness is defined and usually assessed through quantitative metrics mapping the notion of fairness into some statistical measure \cite[e.g., disparate impact, equal opportunity; see][for further details]{wang2022brief}. However, statistical metrics often fail to capture the underlying causal relationships between variables. 
This lack of understanding can limit the effectiveness of interventions aimed at addressing fairness issues. 

This has led to the definition of \emph{counterfactual fairness} metrics, considering what would have happened to individuals or groups under different conditions or interventions, even if those conditions or interventions did not actually occur. Counterfactual fairness is a notion of AI equality derived from Pearl's causal framework \cite{pearl2009causality}, stating that a model prediction for an individual is deemed fair when it would not change in the real world if the individual would belong to a different demographic group \cite{NIPS2017_a486cd07,ijcai2019p199}. Counterfactuals provide a framework for reasoning about causality through the comparison of what actually happened with what hypothetically could have happened under different conditions, thus enabling a deeper impact assessment and more structured interventions. 
%
Counterfactuals have been envisaged as model-agnostic tools to provide human-interpretable explanations by identifying the most pivotal features for the ML model decisions, thereby offering insights into the decision-system behaviour \cite{10.1145/3236009,wachter2017counterfactual}. Counterfactual explanations formulate a series of statements aimed at informing users about potential changes to the original input to achieve a different outcome \cite{byrne2007rational}. Explanations based on counterfactuals can thus lead to a counterfactual fairness analysis \cite{artelt2019computation,karimi2021survey,guidotti2022counterfactual,verma2020counterfactual}.

Even though the approach is very promising, to our knowledge, currently there are only a few works dedicated to the intersection between fairness and causality or counterfactual explanations.
%
Some directly exploit the causal graph to assess discrimination. Specifically, \citet{Zhang_Bareinboim_2018} introduced three measures to capture the causal mechanisms that could lead to discrimination, while \citet{Chiappa_2019} proposed a definition of counterfactual fairness based on comparing the pathways to the decision in the causal graph. Similarly, \citet{pan2021explaining} highlights disparities by analysing the causal graph paths linking sensitive attributes to the final predictions. However, causal relationships may be uncertain or difficult to model in dynamic and complex environments, making these methods challenging to apply in real-world scenarios. Therefore, counterfactual approaches are preferable.

Concerning counterfactual approaches for fairness, \citet{10.1609/aaai.v37i9.26344} studied fairness in relation to explanation quality and defined a measure of fairness by comparing the difference in quality for distinct demographic groups. However, how to formalise the explanation quality is still an open issue.
%
Differently, \citet{goethals2023precof} introduced a metric to identify individual unfairness within a predictive classification model focused on detecting explicit bias by comparing factual and counterfactual instances that are identical in terms of non-sensitive features.
%
Our work extends this approach by defining a stronger fairness notion and by applying the concept of explicit bias also to the realm of group fairness.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Counterfactual Explanations for Fairness}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Individual fairness focuses on similarly treating similar individuals. This ensures that they receive analogous predictions or treatments from the AI system regardless of their sensitive attributes. On the other hand, group fairness aims to ensure equal treatment for different groups, such as racial or gender groups, avoiding systematic biases or disparities \cite{ChakrabortyPM20,10.1145/2090236.2090255}.
%
Referring back to our motivating example, individual fairness considers John, Mark, Clara, and Mary as distinct individuals. Group fairness considers the protected groups to which they belong, e.g., it compares how the \emph{male} group (John and Mark) is treated with respect to the \emph{female} group (Mary and Clara), even though individual disparities within each group may be present.

Whether privileging individual or group fairness depends on the context and objectives of the specific AI system. Group fairness addresses broader societal inequalities and a balanced approach considering both fairness concepts may be necessary to ensure equity in AI decision systems.

\paragraph{Notation}
% 
Let $\boldsymbol{z}_i = (\boldsymbol{x}_i, \boldsymbol{a}_i) \in \mathcal{Z}$ represent the $i$-th individual of domain $\mathcal{Z}$ containing non-protected feature values $\boldsymbol{x}_i \in \mathcal{X}$ and protected feature values $\boldsymbol{a}_i \in \mathcal{A}$.
%
Let $\hat{Y}$ denote a predictive function such that $\hat{Y}(\boldsymbol{x}_i, \boldsymbol{a}_i) = \hat{y}_i$, being $\hat{y}_i$ the prediction for instance $\boldsymbol{z}_i$.

Let $\mathcal{Z}^{\boldsymbol{s}}_o$ be the subset of domain $\mathcal{Z}$ where individuals belong to the protected group defined by $\boldsymbol{s}$ and receiving outcome $o$. In other words, the sensitive attributes $\boldsymbol{a}_i$ assume the values $\boldsymbol{s}$, e.g., $\mathcal{Z}^{female}_1, \mathcal{Z}^{male}_1$ if gender is the only protected attribute and we want to consider gender-based groups receiving a positive outcome.
%

Let $\boldsymbol{z}_i^c = (\boldsymbol{x}_i^c, \boldsymbol{a}_i^c) \in \mathcal{Z}$ represent a counterfactual for a given individual $\boldsymbol{z}_i$ with respect to a predictive model $\hat{Y}$, where $\boldsymbol{x}_i^c \in \mathcal{X}$ and $\boldsymbol{a}_i^c \in \mathcal{A}$. Specifically, $\boldsymbol{z}_i^c$ is \emph{minimally different} from $\boldsymbol{z}_i$ and $\hat{Y}(\boldsymbol{z}_i^c) \neq \hat{Y}(\boldsymbol{z}_i)$.

The definition of \emph{minimal difference} depends on the problem domain and its features. Generally, this concept refers to making the smallest possible changes to the original, factual instance to receive a different outcome. Therefore, counterfactuals provide insights into how small input perturbations lead to different predictions. From the fairness assessment standpoint, the notion of minimal difference should only consider non-protected attributes, e.g., the dissimilarity between two individuals should only be based on their skills and abilities and not on their ethnicity and gender.

\paragraph{On similarity assessment}

In the context of fairness measurement through counterfactual explanations, a formal definition of \emph{individual similarity} ($\sim$) is still missing in the literature. We propose the following to fill this gap.
%
\begin{definition}[Threshold-based similarity $\sim_\varepsilon$]\label{similar_individuals}
	Let $d: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}^{+}$ be a distance function defined in the feature domain, representing the dissimilarity between two individuals $\boldsymbol{z}_i = (\boldsymbol{x}_i, \boldsymbol{a}_i)$ and $\boldsymbol{z}_j = (\boldsymbol{x}_j, \boldsymbol{a}_j)$.
	$\boldsymbol{z}_i$ and $\boldsymbol{z}_j$ are similar if the distance between their non-protected attribute values is smaller than a predefined threshold $\varepsilon$:
	$$ \boldsymbol{z}_i \sim_\varepsilon \boldsymbol{z}_j \iff d(\boldsymbol{x}_i, \boldsymbol{x}_j) < \varepsilon. $$
\end{definition}
%
\noindent This definition implies that two individuals can be considered similar if they are ``close enough'' in the input space, regardless of their sensitive features. This aligns with fairness principles, as sensitive features should not influence the determination of similarity between two individuals who should be treated equally by automated decision-making systems.
%
The determination of the $\varepsilon$ threshold and the selection of the proper $d$ function should consider the specific task requirements, data peculiarities and problem domain. Common distance metrics include Euclidean and Manhattan distances and cosine similarity, amongst others.

Inspired by the concept of \emph{consistency} introduced by \cite{ZemelWSPD13}, it is possible to define a neighbourhood-based similarity as follows.
%
\begin{definition}[Neighbourhood-based similarity $\sim_k$]\label{similar_individuals_neigh}
	Let $d: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}^{+}$ be a distance function defined in the feature domain, representing the dissimilarity between two individuals $\boldsymbol{z}_i = (\boldsymbol{x}_i, \boldsymbol{a}_i)$ and $\boldsymbol{z}_j = (\boldsymbol{x}_j, \boldsymbol{a}_j)$.
	The $k$-neighbourhood of $\boldsymbol{z}_i$, denoted as $\mathcal{K}^{\boldsymbol{z}_i}$, contains the $k$ individuals $\boldsymbol{z}_j$ minimising the distance from the non-protected attribute values of $\boldsymbol{z}_i$:
	$$ \boldsymbol{z}_i \sim_k \boldsymbol{z}_j \iff \boldsymbol{z}_j \in \mathcal{K}^{\boldsymbol{z}_i}, $$
	%where
	$$\mathcal{K}^{\boldsymbol{z}_i} \subseteq \mathcal{Z} : ~ | \mathcal{K}^{\boldsymbol{z}_i} | = k,$$
	%$$\hat{y}_j = \hat{y}_i, ~ \forall \boldsymbol{z}_j \in \mathcal{K}^{\boldsymbol{z}_i},$$
	$$d(\boldsymbol{x}_i, \boldsymbol{x}_l) \geq \underset{\boldsymbol{z}_j \in \mathcal{K}^{\boldsymbol{z}_i}}{\max} d(\boldsymbol{x}_i, \boldsymbol{x}_j), ~ \forall \boldsymbol{z}_l \in \mathcal{Z} \setminus \mathcal{K}^{\boldsymbol{z}_i}$$ % : ~ \hat{y}_k = \hat{y}_i.$$
\end{definition}
%
\noindent In other words, all individuals outside the $k$-neighbourhood of $\boldsymbol{z}_i$ have larger distance from $\boldsymbol{z}_i$ than any of the $k$ individuals inside its $k$-neighbourhood and the sensitive attributes are not taken into account in the distance determination.

%--------------------------------------------------------------------
\subsection{Individual Fairness Based on Counterfactuals}
%--------------------------------------------------------------------

According to the current literature, an \emph{explicit bias} is present in a predictive model if its counterfactual explanations imply that at least one protected attribute should be changed to obtain different outcomes \cite{sokol2022fat}. This definition partially superposes with the concept of \emph{counterfactual fairness} \cite{NIPS2017_a486cd07}.

\begin{definition}[Individual fairness: explicit bias]\label{explicit_bias}
	A decision system based on the predictive function $\hat{Y}$ and providing the outcomes $\boldsymbol{\hat{y}}$ for the individuals $\boldsymbol{z}_i = (\boldsymbol{x}_i, \boldsymbol{a}_i)$ with non-protected attribute values $\boldsymbol{x}_{i}$ and protected attribute values $\boldsymbol{a}_{i}$ belonging to the domain $\mathcal{Z}$ is \emph{explicitly biased} if the counterfactuals $\boldsymbol{z}_i^c = (\boldsymbol{x}_i^c, \boldsymbol{a}_i^c)$ have different values for some sensitive variables of $\boldsymbol{z}_i$, for at least one of the factual individuals, i.e., if $\exists \boldsymbol{z}_i \in \mathcal{Z} : ~ \boldsymbol{a}_{i} \neq \boldsymbol{a}_{i}^{c}$.
\end{definition}

The concept of explicit bias can be expanded by introducing the definition of \emph{unequal treatment} at the individual level based on counterfactuals.
%
A predictor providing different counterfactual explanations for a pair of similar individuals, receiving the same outcome,\footnote{We point out here that the unfair situation where two similar individuals receive a different outcome is beyond the scope of this work based on counterfactual fairness} exhibits an unequal treatment for the individuals.
%
This definition ensures that a predictor is fair only if similar individuals can change their outcomes by satisfying the same conditions, thereby preventing unequal treatment as mandated by the principle of non-discrimination in Article 21 of the EU Charter of Fundamental Rights.

\begin{definition}[Individual fairness: unequal treatment]\label{unequal_treatment}
	A decision system based on the predictive function $\hat{Y}$ and providing the outcomes $\boldsymbol{\hat{y}}$ is responsible of \emph{unequal treatment} if two individuals $\boldsymbol{z}_i = (\boldsymbol{x}_i, \boldsymbol{a}_i)$ and $\boldsymbol{z}_j = (\boldsymbol{x}_j, \boldsymbol{a}_j)$, with non-protected attribute values $\boldsymbol{x}_i, \boldsymbol{x}_j$ and protected attribute values $\boldsymbol{a}_i, \boldsymbol{a}_j$, respectively, are similar and receive the same outcome, but have different counterfactuals, i.e., if $\exists \boldsymbol{z}_i, \boldsymbol{z}_j \in \mathcal{Z} : ~ \boldsymbol{z}_i \sim \boldsymbol{z}_j \wedge \hat{y}_i = \hat{y}_j \wedge \boldsymbol{z}_i^c \neq \boldsymbol{z}_j^c$.
\end{definition}

Returning to the motivating example, individual fairness via explicit bias in the counterfactuals ensures that John can obtain the loan without altering any of his sensitive attributes, e.g., he is not required to change his gender to female.
%
Differently, the assessment of individual fairness through the analysis of unequal treatment ensures that the loan is granted to both John and Mary under the same conditions, e.g., if they can increase their income. For instance, it ensures that John is not asked to increase his income while Mary is also required to have an additional qualification.
%
The assessment of unequal treatment should be conducted for all individuals who are similar to John and receive the same outcome, regardless of their sensitive attributes.

Finally, we present the definition of \emph{comprehensive individual fairness} by merging \Cref{explicit_bias,unequal_treatment}, as follows.
%
\begin{definition}[Individual comprehensive fairness]\label{comprehensive}
	A decision system based on the predictive function $\hat{Y}$ is \emph{comprehensively individual-fair} if its outcomes are explicitly unbiased and exhibit equal treatment for similar individuals, i.e., $\boldsymbol{a}_i = \boldsymbol{a}_i^c \wedge \boldsymbol{z}_i^c = \boldsymbol{z}_j^c, \forall \boldsymbol{z}_i, \boldsymbol{z}_j \in \mathcal{Z} : ~ \boldsymbol{z}_i \sim \boldsymbol{z}_j \wedge \hat{y}_i = \hat{y}_j $.
\end{definition}

\subsection{Group Fairness Based on Counterfactuals}

\Cref{explicit_bias,unequal_treatment} can be extended to carry out group fairness evaluations based on counterfactuals.

We define the concept of explicit bias for a group identified by the values $\boldsymbol{s}$ of the protected attributes by checking whether the explicit bias for individuals belonging to that specific group is different to the bias for individuals within a different group.
%
The notion of individual bias is the same given in \Cref{explicit_bias}.
%
\begin{definition}[Group fairness: explicit bias]\label{explicit_bias_group}
	A decision system based on the predictive function $\hat{Y}$ and providing the outcomes $\boldsymbol{\hat{y}}$ for the individuals $\boldsymbol{z}_i = (\boldsymbol{x}_i, \boldsymbol{a}_i)$ with non-protected attribute values $\boldsymbol{x}_{i}$ and protected attribute values $\boldsymbol{a}_{i}$ belonging to the domain $\mathcal{Z}$ is \emph{explicitly biased} if the counterfactuals $\boldsymbol{z}_i^c = (\boldsymbol{x}_i^c, \boldsymbol{a}_i^c)$ for factual individuals belonging to the group $\mathcal{Z}^{\boldsymbol{s}}_o$ have a different probability to contain different values for some sensitive variables of $\boldsymbol{z}_i$ with respect to the counterfactuals for individuals of another group $\mathcal{Z}^{\boldsymbol{s}'}_o$, i.e., if $\exists \mathcal{Z}^{\boldsymbol{s}}_o, \mathcal{Z}^{\boldsymbol{s}'}_o \subseteq \mathcal{Z} : ~ \boldsymbol{s} \neq \boldsymbol{s}' \wedge \mathbb{P}(\boldsymbol{a}_{i} \neq \boldsymbol{a}_{i}^{c} \mid \boldsymbol{z}_i \in \mathcal{Z}^{\boldsymbol{s}}_o) \neq \mathbb{P}(\boldsymbol{a}_{j} \neq \boldsymbol{a}_{j}^{c} \mid \boldsymbol{z}_j \in \mathcal{Z}^{\boldsymbol{s}'}_o)$.
\end{definition}

The notion of unequal treatment for a group identified by the values $\boldsymbol{s}$ of the protected attributes and outcome $o$ is formalised by comparing the counterfactual explanations for the individuals in the protected group with those generated for similar individuals belonging to the other groups who received the same outcomes.
%
Unequal treatment is detected when such pairs of similar individuals receive different counterfactuals.
%
\begin{definition}[Group fairness: unequal treatment]\label{unequal_treatment_group}
	A decision system based on the predictive function $\hat{Y}$ and providing the outcomes $\boldsymbol{\hat{y}}$ is responsible of \emph{unequal treatment} with respect to group $\mathcal{Z}^{\boldsymbol{s}}_o$ if for an individual $\boldsymbol{z}_i = (\boldsymbol{x}_i, \boldsymbol{a}_i)$ belonging to that group, with non-protected attribute values $\boldsymbol{x}_i$ and protected attribute values $\boldsymbol{a}_i$, it is possible to find a similar individual belonging to a different group $\mathcal{Z}^{\boldsymbol{s}'}_o$, who receives the same outcome but a different counterfactual, i.e., if $\exists \boldsymbol{z}_i \in \mathcal{Z}^{\boldsymbol{s}}_o, \boldsymbol{z}_j \in \mathcal{Z}^{\boldsymbol{s}'}_o : ~ \boldsymbol{s} \neq \boldsymbol{s}' \wedge \boldsymbol{z}_i \sim \boldsymbol{z}_j \wedge \hat{y}_i = \hat{y}_j = o \wedge \boldsymbol{z}_i^c \neq \boldsymbol{z}_j^c$.
\end{definition}

About the motivating example, counterfactual group fairness via explicit bias ensures that the individual bias for female applicants (e.g., Mary) is not stronger than the bias for male applicants (e.g., John). This is not equivalent to an absolute absence of individual bias, but it highlights that the predictive system is not making decisions to penalise a specific group.
%
On the other hand, by assessing group fairness through unequal treatment it is possible to ensure that similar individuals of different groups who were denied the loan (e.g., John and Mary) can obtain it by fulfilling the same requirements, without receiving different treatments based on their group belonging.

The notions formalised in \Cref{explicit_bias_group,unequal_treatment_group} can be merged in the concept of \emph{comprehensive group fairness}, as follows.
%
\begin{definition}[Group comprehensive fairness]\label{comprehensive_group}
	A decision system based on the predictive function $\hat{Y}$ is \emph{comprehensively group-fair} if the explicit bias of its outcomes is homogeneous across the represented groups and if it exhibits equal treatment for similar individuals belonging to different groups, i.e., $\mathbb{P}(\boldsymbol{a}_{i} \neq \boldsymbol{a}_{i}^{c}) = \mathbb{P}(\boldsymbol{a}_{j} \neq \boldsymbol{a}_{j}^{c}) \wedge \boldsymbol{z}_i^c = \boldsymbol{z}_j^c, ~ \forall \boldsymbol{z}_i \in \mathcal{Z}^{\boldsymbol{s}}_o, \boldsymbol{z}_j \in \mathcal{Z}^{\boldsymbol{s}'}_o : ~ \boldsymbol{s} \neq \boldsymbol{s}' \wedge \boldsymbol{z}_i \sim \boldsymbol{z}_j \wedge \hat{y}_i = \hat{y}_j = o$.
\end{definition}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Fairness Metrics Based on Counterfactuals}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Given the definitions provided in the previous section, we propose here computational fairness metrics to quantitatively assess the extent of individual and group fairness of a decision system through the generation of counterfactuals.
%
All metric scores are expressed in the [0, 1] interval, with values towards 1 identifying unfair decisions.

\paragraph{$\Phi_b^i$} [individual explicit bias, cf.\ \Cref{explicit_bias}] ratio of individuals whose counterfactual changes at least one sensitive attribute over all the individuals of the domain:
%
$$\Phi_b^i = \frac{ \mid \left\{ \boldsymbol{z}_i \in \mathcal{Z} : \boldsymbol{a}_{i} \neq \boldsymbol{a}_{i}^{c} \right\} \mid }{\mid \mathcal{Z} \mid} $$

\paragraph{$\Phi_t^i$} [individual unequal treatment, cf.\ \Cref{unequal_treatment}] ratio of individuals for which exists at least one similar individual receiving the same outcome but having a different counterfactual over all the individuals of the domain:
%
$$\small \Phi_t^i = \frac{ \mid \left\{ \boldsymbol{z}_i \in \mathcal{Z} : \exists \boldsymbol{z}_j \in \mathcal{Z} \text{ s.t. } \boldsymbol{z}_i \sim \boldsymbol{z}_j \wedge \hat{y}_i = \hat{y}_j \wedge \boldsymbol{z}_i^c \neq \boldsymbol{z}_j^c \right\} \mid }{\mid \mathcal{Z} \mid} $$
	
	
\paragraph{$\Phi_b^g$} [group explicit bias, cf.\ \Cref{explicit_bias_group}] difference between the maximum and minimum ratios of explicit biases $\phi_b^{\boldsymbol{s},o}$, calculated for all the represented groups denoted by $\boldsymbol{s}$ values and $o$ outcomes:
%
$$\phi_b^{\boldsymbol{s},o} = \frac{ | \left\{ \boldsymbol{z}_i \in \mathcal{Z}^{\boldsymbol{s}}_o : \boldsymbol{a}_{i} \neq \boldsymbol{a}_{i}^{c} \right\} | }{\mid \mathcal{Z}^{\boldsymbol{s}}_o \mid}$$
%
$$\Phi_b^g = \underset{\boldsymbol{s},o}{\max} \left\{ \phi_b^{\boldsymbol{s},o} \right\} - \underset{\boldsymbol{s},o}{\min} \left\{ \phi_b^{\boldsymbol{s},o} \right\} $$
	
\paragraph{$\Phi_t^g$} [group unequal treatment, cf.\ \Cref{unequal_treatment_group}] maximum ratio of unequal treatment $\phi_t^{\boldsymbol{s}, \boldsymbol{s}',o}$, calculated for all pairs of the represented groups denoted by $\boldsymbol{s}$ and $\boldsymbol{s}'$ values and $o$ outcomes:
%
$$\phi_t^{\boldsymbol{s}, \boldsymbol{s}', o} = \frac{\mid \{ \boldsymbol{z}_i \in \mathcal{Z}^{\boldsymbol{s}}_o : \exists \boldsymbol{z}_j \in \mathcal{Z}^{\boldsymbol{s}'}_o\text{s.t. } \boldsymbol{z}_i \sim \boldsymbol{z}_j \wedge \boldsymbol{z}_i^c \neq \boldsymbol{z}_j^c \} \mid}{\mid \mathcal{Z}^{\boldsymbol{s}}_o \mid}$$
%
$$\Phi_t^g = \underset{\boldsymbol{s}, \boldsymbol{s}', o : ~ \boldsymbol{s} \neq \boldsymbol{s}'}{\max} \left\{ \phi_t^{\boldsymbol{s}, \boldsymbol{s}', o} \right\}$$

\noindent Comprehensive fairness metrics derived from \Cref{comprehensive,comprehensive_group} can be easily obtained by merging the numerators in the above formulations.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiments}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Experiments demonstrating the effectiveness of the fairness metrics proposed in this work are reported in the following.
%
We focus on a tabular data set commonly used as a benchmark in the literature about ML fairness, namely, the Adult Income data set \cite{Dua_2019}.
%
The data set provides the results of a census made in 1994 in the United States. Specifically, it contains information about 48\,842 individuals, described through 8 categoric and 6 numeric features. Some instances present missing data.
%
The objective of this data set is to predict whether an individual's annual income is above or below $50\,000\$$, considering factors such as occupation, marital status, and education. The data set contains demographic attributes, e.g., age, gender, and race.
%
Therefore, it has gained significant popularity within the algorithmic fairness community as a reliable benchmark for evaluating and comparing the effectiveness of fair ML techniques.
%
%The data set feature characteristics are summarized in \Cref{tab:adult}.

%\begin{table*}[t!]
%	\begin{center}{\caption{Characteristics of the Adult data set features.}\label{tab:adult}}
%		\begin{tabular}{lllc}
%			\toprule
%			Feature name & Domain & Description & Missing values \\
%			\midrule					
%			age & Numeric, integer (17 -- 90) & Age & \\
%			workclass & Categoric (8 values) & Employment status & $\times$ \\
%			fnlwgt & Numeric, integer (12\,285 -- 1\,490\,400) & Individual representativeness & \\
%			education & Categoric (16 values) & Highest education level & \\
%			education-num & Numeric, integer (1 -- 16) & Highest education level & \\
%			marital-status & Categoric (7 values) & Marital status & \\
%			occupation & Categoric (14 values) & General occupation class & $\times$ \\
%			relationship & Categoric (6 values) & Relationship with other individuals & \\
%			race & Categoric (5 values) & Race & \\
%			sex & Categoric (2 values) & Gender & \\
%			capital-gain & Numeric, integer (0 -- 99\,999) & Capital gain & \\
%			capital-loss & Numeric, integer (0 -- 4\,356) & Capital loss & \\
%			hours-per-week & Numeric, integer (1 -- 99) & Working hours per week & \\
%			native-country & Categoric (41 values) & Origin country & $\times$ \\
%			\midrule
%			income & Categoric (2 values) & Income & \\
%			\bottomrule
%		\end{tabular}
%	\end{center}
%\end{table*}

% Our fairness experiments employ a set of opaque classifiers trained on the Adult Income data set, i.e., a gradient boosting (GB), a K-nearest neighbor (KNN) and a support vector machine (SVM) whose hyper-parameters are resumed in \Cref{tab:bb}.

%\begin{table}[t!]
%\scriptsize{
%	\begin{center}{\caption{Performance of the black-box (BB) predictors employed in our experiments. The fidelity achieved by the knowledge extractors applied to the BBs is also reported. Average values over 10 runs are reported.}\label{tab:bb}}
%		\begin{tabular}{lll}
%			\toprule
%			BB & BB accuracy & Extractor fidelity  \\
%			\midrule					
%			GB & 0.85 $\pm$ 0.00 & 0.88 $\pm$ 0.00 \\
%			\midrule
%			KNN & 0.82 $\pm$ 0.00 & 0.90 $\pm$ 0.00 \\
%			\midrule
%			SVM & 0.83 $\pm$ 0.00 & 0.88 $\pm$ 0.00 \\
%			\bottomrule
%		\end{tabular}
%	\end{center}
%}
%\end{table}

\paragraph{Data preparation and model training}

Our experiments employ a set of opaque classifiers trained on the Adult Income data set: a gradient boosting (GB), a K-nearest neighbour (KNN) and a support vector machine (SVM). 
%
The GB adopted 200 base estimators and a learning rate of 0.2.
%
The KNN had a K parameter equal to 150 and used Euclidean distance and uniform weights.
%
The SVM employed a radial basis function kernel with regularisation parameter of 0.85.
%
It is worth noting that the chosen classifiers do not incorporate any algorithmic fairness constraints in their training process.
%
A pre-processing of the data set has been performed, consisting of a mapping of categoric features into numeric ones followed by a normalisation of all inputs, given that the variables range in very different domains.
%
In detail, categorical features for sex and race were converted into integer values between 0 and 1 or 4, respectively. All features were then standardised to have a mean of 0 and a variance of 1.
%
The resulting data set was split into training and test sets with a 50\%:50\% ratio.
%
The training set was used for hyper-parameter tuning of the black-box classifiers via a grid search with 3-fold cross-validation.
%
After tuning, the classifiers, parametrised with the best-found values, were trained on the entire training set.
%
The resulting accuracy scores were consistent with the baseline performance established by the Adult data set benchmarks (see \Cref{tab:res}).

\paragraph{Counterfactual generation via knowledge extraction}

Counterfactual explanations have been generated through symbolic knowledge extraction \cite{SKESKISLR2024}.
%
We leveraged extraction algorithms implemented within the PSyKE Python framework \cite{psyke-trust-aixia2022}. In particular, we adopted the GridEx algorithm to obtain symbolic knowledge mimicking the predictive behaviour of the three aforementioned black boxes.%\cite{gridex-extraamas2021}
%
We employed the fairness metrics presented in this work to assess the extent of fairness exhibited by the classification process.

We recall here that GridEx recursively partitions the input feature space in a set of disjoint hypercubic subregions that may be described in terms of interval-inclusion constraints on the data set input features.
Each region is associated with a class label, representing an interpretable prediction.
The execution of GridEx requires the tuning of a set of hyper-parameters, namely: the recursion depth, set to 1, the minimum quantity of training samples to consider within each identified hypercubic region, set to 100, and the strategy to follow for the input space partitioning.
%
An adaptive splitting strategy aware of the input feature relevance has been preferred over a fixed strategy leading to the indiscriminate partitioning of every input feature regardless of their importance. More in detail, we selected GridEx instances performing no slices along input dimensions with relevance below 0.15 and obtaining, conversely, 2 or 3 partitions for dimensions with importance above 0.15 or 0.90, respectively\footnote{The input feature importance may be estimated via arbitrary methods, as long as these estimates are provided to GridEx normalised in the [0, 1] interval.}.
%
These optimum values have been set via the dedicated PEDRO procedure implemented within PSyKE. % \cite{gridrex-kr2022}.
%
The result of the knowledge-extraction process is a set of symbolic rules expressed in Prolog format, summarising the rationale behind the classification in a human-interpretable way.
%
An example is shown in \Cref{lst:rules}.

\begin{table*}[t!]
%\scriptsize{
	\begin{center}{\caption{Experiment results. $f$ and $m$ denote individual groups based on female and male sex, respectively. $+$ and $-$ denote positive and negative outcomes, respectively. Accuracy of the black boxes and fidelity of knowledge extractors are also reported.}\label{tab:res}}
		\begin{tabular}{ll|c|c|c}
			\toprule
			Metrics $\downarrow$ & Black box $\rightarrow$ & GB & KNN & SVM \\
			\midrule
			Black-box accuracy & & 0.85 $\pm$ 0.00 & 0.82 $\pm$ 0.00 & 0.83 $\pm$ 0.00 \\
			Extractor fidelity & & 0.88 $\pm$ 0.00 & 0.90 $\pm$ 0.00 & 0.88 $\pm$ 0.00 \\
			\midrule
			Individual fairness: & & & & \\
			~ ~ Fairness through awareness & & 0.92 $\pm$ 0.00 & 0.94 $\pm$ 0.00 & 0.95 $\pm$ 0.00 \\
			~ ~ Counterfactual fairness & & 0.89 $\pm$ 0.00 & 0.87 $\pm$ 0.00 & 0.89 $\pm$ 0.00 \\
			~ ~ Explicit bias & $\Phi_b^i$ & 0.31 $\pm$ 0.00 & 0.30 $\pm$ 0.04 & 0.33 $\pm$ 0.00 \\
			~ ~ Unequal treatment & $\Phi_t^i$ & 0.38 $\pm$ 0.00 & 0.40 $\pm$ 0.00 & 0.40 $\pm$ 0.00 \\
			\midrule
			Group fairness: & & & & \\
			~ ~ Disparate impact & & 0.17 $\pm$ 0.00 & 0.08 $\pm$ 0.01 & 0.14 $\pm$ 0.01 \\
			~ ~ Explicit bias & $\Phi_b^g$ & 0.63 $\pm$ 0.08 & 0.91 $\pm$ 0.12 & 1.00 $\pm$ 0.00 \\
			~ ~ ~ ~ Male group, positive outcome & $\phi_b^{m,+}$ & 0.33 $\pm$ 0.00 & 0.03 $\pm$ 0.00 & 0.03 $\pm$ 0.00 \\
			~ ~ ~ ~ Female group, positive outcome & $\phi_b^{f,+}$ & 0.42 $\pm$ 0.02 & 0.36 $\pm$ 0.13 & 0.98 $\pm$ 0.01 \\
			~ ~ ~ ~ Male group, negative outcome & $\phi_b^{m,-}$ & 0.07 $\pm$ 0.03 & 0.00 $\pm$ 0.00 & 0.00 $\pm$ 0.00 \\
			~ ~ ~ ~ Female group, negative outcome & $\phi_b^{f,-}$ & 0.70 $\pm$ 0.05 & 0.91 $\pm$ 0.12 & 1.00 $\pm$ 0.00 \\
			~ ~ Unequal treatment & $\Phi_t^g$ & 1.00 $\pm$ 0.00 & 1.00 $\pm$ 0.00 & 1.00 $\pm$ 0.00 \\
			~ ~ ~ ~ Male group versus female group, positive outcome & $\phi_t^{m,f,+}$ & 1.00 $\pm$ 0.00 & 1.00 $\pm$ 0.00 & 1.00 $\pm$ 0.00 \\
			~ ~ ~ ~ Female group versus male group, positive outcome & $\phi_t^{f,m,+}$ & 1.00 $\pm$ 0.00 & 1.00 $\pm$ 0.00 & 1.00 $\pm$ 0.00 \\
			~ ~ ~ ~ Male group versus female group, negative outcome & $\phi_t^{m,f,-}$ & 1.00 $\pm$ 0.00 & 1.00 $\pm$ 0.00 & 1.00 $\pm$ 0.00 \\
			~ ~ ~ ~ Female group versus male group, negative outcome & $\phi_t^{f,m,-}$ & 1.00 $\pm$ 0.00 & 1.00 $\pm$ 0.00 & 1.00 $\pm$ 0.00 \\
			\bottomrule
		\end{tabular}
	\end{center}
%}
\end{table*}

\begin{listing*}[t!]
	\caption{Example of knowledge extracted with GridEx for the Adult Income data set. Knowledge expressed as a Prolog theory.}\label{lst:rules}
	\begin{lstlisting}[]
income(Age, CapitalGain, CapitalLoss, EducationNum, Fnlwgt, HoursPerWeek, Race, Sex, '>50K') 
    :- EducationNum > 11, Sex = 'Male', CapitalGain =< 33332.
income(Age, CapitalGain, CapitalLoss, EducationNum, Fnlwgt, HoursPerWeek, Race, Sex, '<=50K') 
    :- EducationNum > 11, Sex = 'Female', CapitalGain =< 33332.
income(Age, CapitalGain, CapitalLoss, EducationNum, Fnlwgt, HoursPerWeek, Race, Sex, '>50K') 
    :- EducationNum =< 5, CapitalGain > 66666.
income(Age, CapitalGain, CapitalLoss, EducationNum, Fnlwgt, HoursPerWeek, Race, Sex, '>50K') 
    :- EducationNum > 11, CapitalGain > 66666.
income(Age, CapitalGain, CapitalLoss, EducationNum, Fnlwgt, HoursPerWeek, Race, Sex, '<=50K') 
    :- EducationNum =< 11, CapitalGain =< 33332.
income(Age, CapitalGain, CapitalLoss, EducationNum, Fnlwgt, HoursPerWeek, Race, Sex, '>50K') 
    :- EducationNum in [6, 11], CapitalGain > 33332.
	\end{lstlisting}
\end{listing*}

Starting from these rules, it is possible to obtain counterfactual explanations for a given individual. With GridEx, the following strategy can be adopted: \textit{(i)} find the hypercubic region enclosing the given individual; \textit{(ii)} identify the region associated with a different class label that is closest to the individual; \textit{(iii)} generate the minimal change required by the given individual to move in the identified closest hypercubic region.
%
The resulting counterfactuals can be used to conduct the fairness measurements presented in this work.

Amongst the properties outlined by \citet{guidotti2022counterfactual}, the following are exhibited by the counterfactuals generated with GridEx: \textit{validity} (counterfactuals are always classified differently from the individuals), \textit{sparsity} (counterfactuals are described by the minimum number of input features to be changed), \textit{similarity} (counterfactuals are as close as possible to the given individuals), \textit{plausibility} (counterfactuals belong to the same data distribution as the individuals), and \textit{discriminative power} (humans can notice the differences between the individuals and their counterfactuals).%even though the corresponding human classification may differ due to diverse biases in human and machine classification.

We emphasise that the counterfactuals provided by GridEx do not consider the actionability property, defined as the absence of sensitive features in the counterfactuals. This is a prerequisite for our fairness study, requiring counterfactual explanations possibly based on sensitive attributes. Additionally, GridEx counterfactuals are model-agnostic.

In general, GridEx may serve as a stable and efficient counterfactual explainer. We adhere to the notions of stability and efficiency as described by \citet{guidotti2022counterfactual}, where an efficient explainer is capable of providing fast responses, while a stable one yields similar counterfactuals when queried with similar instances.
%
It is important to note that fairness is not guaranteed by the design in GridEx; rather, it reflects the same level of fairness as the black-box classifier it explains. Therefore, GridEx can be used as a tool to assess the fairness extent of its underlying opaque predictors.

%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion and Conclusions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
The experiments conducted in this study consider sex and race as sensitive attributes, as described in the Adult Income dataset. However, since race does not significantly influence the counterfactuals or the classification process (we observed no differences based on racial subgroups), the results in \Cref{tab:res} refer only to gender and are not grouped based on race. These findings can also be verified by analysing \Cref{lst:rules}. The first two rules highlight that female and male individuals with the same characteristics (education and income) receive different outcomes based on their sex, while race is never considered in determining the outcome.

All results were averaged over 10 runs, each with different train/test splits and, consequently, different black-box model training.
%
As a result, the corresponding extraction techniques were also trained each time with a different set of instances.
%
Comprehensive results of our experiments are summarised in \Cref{tab:res}.

\paragraph{Individual fairness results}
The results of our experiments were compared with two widely used state-of-the-art metrics for individual fairness, namely fairness through awareness \cite{10.1145/2090236.2090255}, which quantifies pairs of similar individuals receiving the same classification, and counterfactual fairness \cite{NIPS2017_a486cd07}, which indicates if individuals receive the same outcome after changing their sensitive attributes.
%
From the fairness measurements, it is notable that the results for KNN are generally less stable (higher standard deviation) than those obtained for the GB and SVM models, yet still aligned with their measurements.
%
Overall, state-of-the-art metrics suggest that all classifiers exhibit a fairness extent around 90\%, considering both fairness through awareness and counterfactual fairness.
%
According to our metrics, the individual explicit bias is around 30\% for all models, whereas the individual unequal treatment is around 40\%.
%
This means that one individual out of three receives a counterfactual based on protected features, while two individuals out of five receive a different counterfactual compared to similar individuals receiving the same outcome.
%
In other words, one individual out of three is required to change sex (possibly amongst other features) to have a decision reversal, and two individuals out of five are treated differently compared to similar individuals, since they can obtain a decision swap by fulfilling different conditions.
%
With reference to the running example, John and Mary are similar (in terms of socio-economic status) and both did not receive the loan (thus fairness through awareness is ensured). Mary is not able to obtain it even if she changes her sex to male (counterfactual fairness is ensured). John can get the loan by increasing his income. However, Mary is required to increase her income and have an additional masters degree that is not required for John. This condition represents explicit bias and unequal treatment with respect to a similar individual.

If we considered only state-of-the-art metrics, this would not be highlighted. It would seem that the predictors are fairly fair, with a bias behavior percentage around 10\%. The percentage of identified bias is significantly higher with our metrics. This aligns with expectations. Fairness through awareness, not working on counterfactuals, indeed measures something completely different, not studying the necessity of impacting sensitive attributes in the decision change or having unequal treatment to swap a decision.
%
Counterfactual fairness, although it works on counterfactuals by focusing on the specific case, does not capture everything detected by our metrics. In fact, its value is contained in our metric for explicit bias, although it is expanded because we generally study whether a decision swap requires a change in a sensitive attribute (specifically gender). Many cases are overlooked by counterfactual fairness, limiting the possible change only to the sensitive attribute under examination.
%
Unequal treatment is not measured by either of the two existing metrics, but it represents a particularly interesting value for adhering to current legislation.

\paragraph{Group fairness results}
For group fairness, we chose to compare using the state-of-the-art disparate impact metric \cite{FeldmanFMSV15}, which represents the proportion of positive classifications across groups.

From a group-level perspective, the disparate impact is very low, while the explicit bias ranges between 63\% and 100\% depending on the classifier. This indicates that positive outcomes are not evenly distributed amongst female and male applicants, nor is the decision bias consistent across the represented groups.
%
Notably, male individuals experience almost no bias at all, regardless of their outcome, whereas female individuals receive strongly biased decisions. Regarding the extent of unequal treatment at the group level, it is evident that any male (female) individual is treated differently from the most similar female (male) individual receiving the same outcome, regardless of the chosen classifier.
%
Overall, a significant bias against female individuals can be detected, highlighting that outcomes from black-box models are predominantly influenced by sensitive attributes. This analysis is confirmed by the knowledge extracted with GridEx, demonstrating that both positive and negative outcomes depend on protected features. This constitutes a clear source of unfairness, accurately captured by our metrics and completely discarded by the disparate impact metric.

All analyses and provided percentages offer an automated calculation basis that can be useful for legal experts to determine whether \emph{prima facie} discrimination can be supported in the case at hand. Therefore, the results are very effective in providing a concise overview of the situation, highlighting scenarios that would not have been evident with traditional methods.


%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Conclusions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%
%This paper tackles the issue of fairness assessment through counterfactual explanations. The analysis of counterfactuals enables a deeper understanding of the potential causes leading to discrimination in decisions made by AI systems.
%
%Novel concepts of counterfactual fairness are introduced, to address both individual and group fairness in automatic decision-making.
%%
%%Our approach aims to foster a better understanding of how sensitive attributes influence decisions in AI systems.
%%
%The work has demonstrated an initial and highly effective experimentation on a well-known data set used as a benchmark for fairness-related issues.
%
%Future works will be devoted to more extensive experimentation, involving data from real-world use cases and validation from stakeholders. 
%%
%An enhancement of the proposed metrics would be relaxing the equality constraint for counterfactuals of similar individuals into a similarity constraint with a threshold.
%%
%The computational cost of group fairness metrics may be enhanced by selecting a set of representative individuals for each group and by computing the fairness scores on them.
%%
%Additionally, more generalised definitions incorporating causality will be explored.


\section{Acknowledgments}

%\bibliographystyle{aaai25}
\bibliography{mybibfile}

\end{document}
